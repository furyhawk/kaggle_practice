{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31b7b3e6",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-12-02T23:09:21.046978Z",
     "iopub.status.busy": "2021-12-02T23:09:21.046162Z",
     "iopub.status.idle": "2021-12-02T23:09:27.257544Z",
     "shell.execute_reply": "2021-12-02T23:09:27.256919Z",
     "shell.execute_reply.started": "2021-11-22T04:23:55.062608Z"
    },
    "papermill": {
     "duration": 6.239422,
     "end_time": "2021-12-02T23:09:27.257707",
     "exception": false,
     "start_time": "2021-12-02T23:09:21.018285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n",
    "import tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453660bd",
   "metadata": {
    "papermill": {
     "duration": 0.011062,
     "end_time": "2021-12-02T23:09:27.281089",
     "exception": false,
     "start_time": "2021-12-02T23:09:27.270027",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <center>NLP Disaster Tweet Classification w/ roBERTa</center>\n",
    "\n",
    "This notebook implements a roBERTa model in Tensorflow to evaluate whether a tweet is about a disaster or not. I have provided explanations throughout to provide a better understanding of what the roBERTa model is actually doing.\n",
    "\n",
    "I got most of my understanding for this notebook from a good discussion thread about the roBERTa model from @Chris Deotte explaining how the components of the model work and his starter notebook on the roBERTa model. These are the top two links below. I also found the Tensorflow documentation quite informative as well (third and fourth links).\n",
    "\n",
    "### Useful Links\n",
    "\n",
    "This is a collection of links that I found helpful in understanding the structure of the roBERTa model, how it works, and more.\n",
    "\n",
    "- [TensorFlow roBERTa Explained Discussion](https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/143281#807401)\n",
    "- [tensorflow-roberta-0-705 Notebook](https://www.kaggle.com/cdeotte/tensorflow-roberta-0-705)\n",
    "- [Bert_en_uncased Docs](https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/4)\n",
    "- [bert_en_uncased_preprocess](https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3)\n",
    "- [How to get meaning from text with language model BERT](https://www.youtube.com/watch?v=-9vVhYEXeyQ)\n",
    "- [TF Bert Tokenizer](https://github.com/google-research/bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c6828c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T23:09:27.308719Z",
     "iopub.status.busy": "2021-12-02T23:09:27.307100Z",
     "iopub.status.idle": "2021-12-02T23:09:27.311061Z",
     "shell.execute_reply": "2021-12-02T23:09:27.310651Z",
     "shell.execute_reply.started": "2021-11-22T04:24:01.325633Z"
    },
    "papermill": {
     "duration": 0.018846,
     "end_time": "2021-12-02T23:09:27.311166",
     "exception": false,
     "start_time": "2021-12-02T23:09:27.292320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#setting a seed for reproducability\n",
    "SEED = 1002\n",
    "def seed_everything(seed):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed) \n",
    "    \n",
    "seed_everything(SEED) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2c9936",
   "metadata": {
    "papermill": {
     "duration": 0.011077,
     "end_time": "2021-12-02T23:09:27.333541",
     "exception": false,
     "start_time": "2021-12-02T23:09:27.322464",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Reading in the data using pandas. We will tokenize the text later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f553425",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T23:09:27.362221Z",
     "iopub.status.busy": "2021-12-02T23:09:27.361714Z",
     "iopub.status.idle": "2021-12-02T23:09:27.446294Z",
     "shell.execute_reply": "2021-12-02T23:09:27.447006Z",
     "shell.execute_reply.started": "2021-11-22T04:24:01.336649Z"
    },
    "papermill": {
     "duration": 0.102016,
     "end_time": "2021-12-02T23:09:27.447148",
     "exception": false,
     "start_time": "2021-12-02T23:09:27.345132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 1: Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n",
      "Tweet 2: Forest fire near La Ronge Sask. Canada\n"
     ]
    }
   ],
   "source": [
    "#reading input data with pandas\n",
    "train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n",
    "test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n",
    "submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\n",
    "\n",
    "#visualizing some of the tweets\n",
    "for i, val in enumerate(train.iloc[:2][\"text\"].to_list()):\n",
    "    print(\"Tweet {}: {}\".format(i+1, val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9664ac06",
   "metadata": {
    "papermill": {
     "duration": 0.012167,
     "end_time": "2021-12-02T23:09:27.471594",
     "exception": false,
     "start_time": "2021-12-02T23:09:27.459427",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <center>bert_encode function</center>\n",
    "\n",
    "\n",
    "### tokenizer\n",
    "We are using the [Tensorflow Research's BERT tokenization method](https://github.com/tensorflow/models/blob/master/official/nlp/bert/tokenization.py). This tokenization method can be thought of as three steps.\n",
    "\n",
    "- Text Normalization\n",
    "    - The first part of the tokenizer converts the text to lowercase (given that we are using the uncased version of roBERTa), converts whitespace to spaces, and strips out accent markers.\n",
    "    ```\n",
    "    \"Alex Pättason's, \"  -> \"alex pattason's,\"\n",
    "    ```\n",
    "    <br></br>\n",
    "- Punctuation splitting\n",
    "    - This next step adds spaces on each side of all \"punctuation\". Note that this includes any non-letter/number/space ASCII characters (ie including \\$, \\@). See more of this in the Docs. \n",
    "    ```\n",
    "    \"Alex Pättason's, \"  -> \"alex pattason ' s ,\"\n",
    "    ```\n",
    "    <br></br>\n",
    "- WordPiece tokenization\n",
    "    - This step applies what is called whitespace tokenization to the output of the process above, and apply's WordPiece tokenization to each word separately. See the example below.\n",
    "    ```\n",
    "    \"Alex Pättason's, \"  -> \"alex pat ##ta ##son ' s ,\"\n",
    "    ```\n",
    "   \n",
    "### tags\n",
    "The next part of the function reduces the length of the text by the max_length that we have specified and adds [CLS] and [SEP] tags to the end of the array. The [CLS] tag is short for classification and indicates the start of the sentence. Similarly, the [SEP] tag indicates the end of the sentence.\n",
    "\n",
    "### convert_tokens_to_ids + pad_masks\n",
    "\n",
    "We then use the tokenizer method to replace the string representation of words with integers. We also create the input mask (AKA pad_masks), and the segment id's. Note that we are not fulfilling the segment_ids full benefits below as we are only passing an array of zeros. More on the tokens, pad_masks, and segment_ids further in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11814c51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T23:09:27.503090Z",
     "iopub.status.busy": "2021-12-02T23:09:27.502401Z",
     "iopub.status.idle": "2021-12-02T23:09:27.505216Z",
     "shell.execute_reply": "2021-12-02T23:09:27.504793Z",
     "shell.execute_reply.started": "2021-11-22T04:24:01.555943Z"
    },
    "papermill": {
     "duration": 0.02141,
     "end_time": "2021-12-02T23:09:27.505351",
     "exception": false,
     "start_time": "2021-12-02T23:09:27.483941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaac6f8e",
   "metadata": {
    "papermill": {
     "duration": 0.011625,
     "end_time": "2021-12-02T23:09:27.528577",
     "exception": false,
     "start_time": "2021-12-02T23:09:27.516952",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <center>build_model function</center>\n",
    "\n",
    "The first three components of the function are basically preprocessing plain text inputs into the input format expected by the roBERTa model.\n",
    "\n",
    "### input_word_ids\n",
    "- Basically maps each word to its token id. There can be multiple different values that correspond with the same word. For example, \"smell\" could be encoded both as 883 and 789.\n",
    "<br></br>\n",
    "```\n",
    "text = \"I love this notebook. It is Great.\"\n",
    "input_word_ids = [10, 235, 123, 938, 184, 301, 567]\n",
    "```\n",
    "\n",
    "### the input_mask\n",
    "- Shows where the sentence begins, and where it ends using an array. All input tokens that are not padding are given a value of 1, and all values that are padding are given 0. If the sentence exceeds that max_length, then the entire vector will be of 1's.\n",
    "<br></br>\n",
    "```\n",
    "text = \"I love this notebook. It is Great.\"\n",
    "input_mask = [1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
    "```\n",
    "\n",
    "### segment_ids\n",
    "- This component is still a little vague for me, but from my understanding, it is recognizing segments of the text. The start of each segment has a 1 in the array, and other components and padding all have a zero. I am unsure as to whether this corresponds to the end of sentences or paragraphs, but if you can explain this better please do so in the comments below!\n",
    "<br></br>\n",
    "```\n",
    "text = \"I love this notebook. It is Great.\"\n",
    "segment_ids = [1, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e4a0cac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T23:09:27.559062Z",
     "iopub.status.busy": "2021-12-02T23:09:27.558226Z",
     "iopub.status.idle": "2021-12-02T23:09:27.560763Z",
     "shell.execute_reply": "2021-12-02T23:09:27.560346Z",
     "shell.execute_reply.started": "2021-11-22T04:24:01.565584Z"
    },
    "papermill": {
     "duration": 0.020661,
     "end_time": "2021-12-02T23:09:27.560872",
     "exception": false,
     "start_time": "2021-12-02T23:09:27.540211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model(bert_layer, max_len=512):\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    #could be pooled_output, sequence_output yet sequence output provides for each input token (in context)\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    out = Dense(1, activation='sigmoid')(clf_output)\n",
    "    \n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    \n",
    "    #specifying optimizer\n",
    "    model.compile(Adam(learning_rate=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5042aa1b",
   "metadata": {
    "papermill": {
     "duration": 0.011478,
     "end_time": "2021-12-02T23:09:27.584081",
     "exception": false,
     "start_time": "2021-12-02T23:09:27.572603",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Build Model + Preprocess Data\n",
    "\n",
    "The first cell below is basically loading in the version of the roBERTa model that we want to use. We are using a Large uncased model. The most simple way to use a roBERTa model and modify it to a specific use case is to set it as a KerasLayer.\n",
    "\n",
    "Note there are many different variations of BERT models that you can look through here --> [TFhub Bert](https://tfhub.dev/google/collections/bert/1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3b55022",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T23:09:27.611915Z",
     "iopub.status.busy": "2021-12-02T23:09:27.611182Z",
     "iopub.status.idle": "2021-12-02T23:09:59.686761Z",
     "shell.execute_reply": "2021-12-02T23:09:59.685782Z",
     "shell.execute_reply.started": "2021-11-22T04:24:01.577209Z"
    },
    "papermill": {
     "duration": 32.091099,
     "end_time": "2021-12-02T23:09:59.686919",
     "exception": false,
     "start_time": "2021-12-02T23:09:27.595820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 23:09:43.873392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-02 23:09:43.968548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-02 23:09:43.969256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-02 23:09:43.970634: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-02 23:09:43.971654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-02 23:09:43.972611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-02 23:09:43.973501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-02 23:09:45.843683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-02 23:09:45.844483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-02 23:09:45.845130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-02 23:09:45.845762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    }
   ],
   "source": [
    "#load uncased bert model\n",
    "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293ee7b2",
   "metadata": {
    "papermill": {
     "duration": 0.012339,
     "end_time": "2021-12-02T23:09:59.711984",
     "exception": false,
     "start_time": "2021-12-02T23:09:59.699645",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In the next cell, we are setting up the tokenizer that will be used to preprocess our input data to what BERT understands. We have to specify a vocab file so that the tokenizer knows what number to encode each word as then we have to specify whether we want uncased or cased text. We will use the same vocab_file that the pre-trained model was trained on (Google's SentencePiece in this case) and we will also use the same case that the model was built for (uncased).\n",
    "\n",
    "Finally, once we have these two variables, we create the tokenizer and tokenize the training and testing data using the bert_encode function that we created above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3017107e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T23:09:59.741985Z",
     "iopub.status.busy": "2021-12-02T23:09:59.741162Z",
     "iopub.status.idle": "2021-12-02T23:10:06.016721Z",
     "shell.execute_reply": "2021-12-02T23:10:06.015772Z",
     "shell.execute_reply.started": "2021-11-22T04:24:32.799539Z"
    },
    "papermill": {
     "duration": 6.292995,
     "end_time": "2021-12-02T23:10:06.016870",
     "exception": false,
     "start_time": "2021-12-02T23:09:59.723875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#vocab file from pre-trained BERT for tokenization\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "\n",
    "#returns true/false depending on if we selected cased/uncased bert layer\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "\n",
    "#Create the tokenizer\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
    "\n",
    "#tokenizing the training and testing data\n",
    "train_input = bert_encode(train.text.values, tokenizer, max_len=160)\n",
    "test_input = bert_encode(test.text.values, tokenizer, max_len=160)\n",
    "train_labels = train.target.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e466eeba",
   "metadata": {
    "papermill": {
     "duration": 0.011624,
     "end_time": "2021-12-02T23:10:06.040998",
     "exception": false,
     "start_time": "2021-12-02T23:10:06.029374",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Having a look at the model summary. We can see the three input layers that we created followed by the roBERTa model which is in the keras_layer. We have the final dense layer which predicts the sentiment of the tweet on a scale of 0-1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2abef4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T23:10:06.074406Z",
     "iopub.status.busy": "2021-12-02T23:10:06.070493Z",
     "iopub.status.idle": "2021-12-02T23:10:07.502332Z",
     "shell.execute_reply": "2021-12-02T23:10:07.501879Z",
     "shell.execute_reply.started": "2021-11-22T04:24:39.375999Z"
    },
    "papermill": {
     "duration": 1.449445,
     "end_time": "2021-12-02T23:10:07.502466",
     "exception": false,
     "start_time": "2021-12-02T23:10:06.053021",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 1024), (None 335141889   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem (Slici (None, 1024)         0           keras_layer[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            1025        tf.__operators__.getitem[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 335,142,914\n",
      "Trainable params: 335,142,913\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(bert_layer, max_len=160)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06e0e72",
   "metadata": {
    "papermill": {
     "duration": 0.012275,
     "end_time": "2021-12-02T23:10:07.527502",
     "exception": false,
     "start_time": "2021-12-02T23:10:07.515227",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training Model\n",
    "\n",
    "The next cell is a simple way of training the model using Keras. We have included the built-in ModelCheckpoint callback to only save the model that has the highest validation accuracy. This ensures we are only saving the best models. \n",
    "\n",
    "\n",
    "We could decrease the randomness of the split by doing some sort of a stratified split, or cross-validation, but this will do for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "200981d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T23:10:07.557455Z",
     "iopub.status.busy": "2021-12-02T23:10:07.556875Z",
     "iopub.status.idle": "2021-12-02T23:31:59.596009Z",
     "shell.execute_reply": "2021-12-02T23:31:59.595433Z",
     "shell.execute_reply.started": "2021-11-22T04:24:40.757232Z"
    },
    "papermill": {
     "duration": 1312.056364,
     "end_time": "2021-12-02T23:31:59.596160",
     "exception": false,
     "start_time": "2021-12-02T23:10:07.539796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 23:10:07.638823: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429/429 [==============================] - 440s 952ms/step - loss: 0.4205 - accuracy: 0.8213 - val_loss: 0.3730 - val_accuracy: 0.8465\n",
      "Epoch 2/3\n",
      "429/429 [==============================] - 407s 948ms/step - loss: 0.2719 - accuracy: 0.8948 - val_loss: 0.4138 - val_accuracy: 0.8596\n",
      "Epoch 3/3\n",
      "429/429 [==============================] - 407s 949ms/step - loss: 0.1518 - accuracy: 0.9425 - val_loss: 0.5273 - val_accuracy: 0.8491\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "train_history = model.fit(\n",
    "    train_input, train_labels,\n",
    "    validation_split=0.1,\n",
    "    epochs=3,\n",
    "    callbacks=[checkpoint],\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4589a2fb",
   "metadata": {
    "papermill": {
     "duration": 0.34438,
     "end_time": "2021-12-02T23:32:00.283597",
     "exception": false,
     "start_time": "2021-12-02T23:31:59.939217",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Make Prediction\n",
    "\n",
    "Using the model to make predictions on the testing set. We round the prediction to 1 or 0. 1 is a disaster tweet, and 0 is a regular tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "176f9ca4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T23:32:00.989295Z",
     "iopub.status.busy": "2021-12-02T23:32:00.988144Z",
     "iopub.status.idle": "2021-12-02T23:33:25.700066Z",
     "shell.execute_reply": "2021-12-02T23:33:25.699115Z",
     "shell.execute_reply.started": "2021-11-22T04:24:50.128319Z"
    },
    "papermill": {
     "duration": 85.076515,
     "end_time": "2021-12-02T23:33:25.700230",
     "exception": false,
     "start_time": "2021-12-02T23:32:00.623715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_pred = model.predict(test_input)\n",
    "\n",
    "submission['target'] = test_pred.round().astype(int)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1456.091891,
   "end_time": "2021-12-02T23:33:29.045237",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-12-02T23:09:12.953346",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
