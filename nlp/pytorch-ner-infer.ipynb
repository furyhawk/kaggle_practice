{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07e90557",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-12-16T11:59:48.856904Z",
     "iopub.status.busy": "2021-12-16T11:59:48.855688Z",
     "iopub.status.idle": "2021-12-16T12:00:01.460260Z",
     "shell.execute_reply": "2021-12-16T12:00:01.461025Z",
     "shell.execute_reply.started": "2021-12-16T04:46:34.267739Z"
    },
    "papermill": {
     "duration": 12.621233,
     "end_time": "2021-12-16T12:00:01.461377",
     "exception": false,
     "start_time": "2021-12-16T11:59:48.840144",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import *\n",
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48ba7943",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T12:00:01.491825Z",
     "iopub.status.busy": "2021-12-16T12:00:01.490690Z",
     "iopub.status.idle": "2021-12-16T12:00:01.494186Z",
     "shell.execute_reply": "2021-12-16T12:00:01.493498Z",
     "shell.execute_reply.started": "2021-12-16T04:46:47.283555Z"
    },
    "papermill": {
     "duration": 0.021366,
     "end_time": "2021-12-16T12:00:01.494336",
     "exception": false,
     "start_time": "2021-12-16T12:00:01.472970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'fold_num': 5, \n",
    "    'seed': 42,\n",
    "    'model': '../input/roberta-base',\n",
    "    'max_len': 512,\n",
    "    'epochs': 5,\n",
    "    'train_bs': 24,\n",
    "    'valid_bs': 32,\n",
    "    'lr': 2e-5,\n",
    "    'num_workers': 0,\n",
    "    'weight_decay': 1e-6,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1d522ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T12:00:01.524385Z",
     "iopub.status.busy": "2021-12-16T12:00:01.523675Z",
     "iopub.status.idle": "2021-12-16T12:00:01.531014Z",
     "shell.execute_reply": "2021-12-16T12:00:01.531573Z",
     "shell.execute_reply.started": "2021-12-16T04:46:47.299057Z"
    },
    "papermill": {
     "duration": 0.025514,
     "end_time": "2021-12-16T12:00:01.531788",
     "exception": false,
     "start_time": "2021-12-16T12:00:01.506274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(CFG['seed'])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5b575ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T12:00:01.558246Z",
     "iopub.status.busy": "2021-12-16T12:00:01.557049Z",
     "iopub.status.idle": "2021-12-16T12:00:01.598299Z",
     "shell.execute_reply": "2021-12-16T12:00:01.598864Z",
     "shell.execute_reply.started": "2021-12-16T04:46:47.384687Z"
    },
    "papermill": {
     "duration": 0.056604,
     "end_time": "2021-12-16T12:00:01.599073",
     "exception": false,
     "start_time": "2021-12-16T12:00:01.542469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "      <th>predictionstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18409261F5C2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D46BCB48440A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D72CB1C11673</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DF920E0A7337</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  class  predictionstring\n",
       "0  18409261F5C2    NaN               NaN\n",
       "1  D46BCB48440A    NaN               NaN\n",
       "2  0FB0700DAF44    NaN               NaN\n",
       "3  D72CB1C11673    NaN               NaN\n",
       "4  DF920E0A7337    NaN               NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('../input/feedback-prize-2021/sample_submission.csv')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0bc86c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T12:00:01.625608Z",
     "iopub.status.busy": "2021-12-16T12:00:01.624624Z",
     "iopub.status.idle": "2021-12-16T12:00:01.670542Z",
     "shell.execute_reply": "2021-12-16T12:00:01.671428Z",
     "shell.execute_reply.started": "2021-12-16T04:46:47.422113Z"
    },
    "papermill": {
     "duration": 0.061165,
     "end_time": "2021-12-16T12:00:01.671718",
     "exception": false,
     "start_time": "2021-12-16T12:00:01.610553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 366.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>[During, a, group, project,, have, you, ever, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D72CB1C11673</td>\n",
       "      <td>[Making, choices, in, life, can, be, very, dif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18409261F5C2</td>\n",
       "      <td>[80%, of, Americans, believe, seeking, multipl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DF920E0A7337</td>\n",
       "      <td>[Have, you, ever, asked, more, than, one, pers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D46BCB48440A</td>\n",
       "      <td>[When, people, ask, for, advice,they, sometime...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text\n",
       "0  0FB0700DAF44  [During, a, group, project,, have, you, ever, ...\n",
       "1  D72CB1C11673  [Making, choices, in, life, can, be, very, dif...\n",
       "2  18409261F5C2  [80%, of, Americans, believe, seeking, multipl...\n",
       "3  DF920E0A7337  [Have, you, ever, asked, more, than, one, pers...\n",
       "4  D46BCB48440A  [When, people, ask, for, advice,they, sometime..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_names, test_texts = [], []\n",
    "for f in tqdm(list(os.listdir('../input/feedback-prize-2021/test'))):\n",
    "    test_names.append(f.replace('.txt', ''))\n",
    "    test_texts.append(open('../input/feedback-prize-2021/test/' + f, 'r').read())\n",
    "test_texts = pd.DataFrame({'id': test_names, 'text': test_texts})\n",
    "test_texts['text'] = test_texts['text'].apply(lambda x:x.split())\n",
    "test_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d222c34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T12:00:01.704474Z",
     "iopub.status.busy": "2021-12-16T12:00:01.703750Z",
     "iopub.status.idle": "2021-12-16T12:00:01.886370Z",
     "shell.execute_reply": "2021-12-16T12:00:01.885751Z",
     "shell.execute_reply.started": "2021-12-16T04:46:47.475285Z"
    },
    "papermill": {
     "duration": 0.202022,
     "end_time": "2021-12-16T12:00:01.886517",
     "exception": false,
     "start_time": "2021-12-16T12:00:01.684495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG['model'], add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba0ec478",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T12:00:01.916691Z",
     "iopub.status.busy": "2021-12-16T12:00:01.916015Z",
     "iopub.status.idle": "2021-12-16T12:00:01.918493Z",
     "shell.execute_reply": "2021-12-16T12:00:01.917991Z",
     "shell.execute_reply.started": "2021-12-16T04:46:47.639633Z"
    },
    "papermill": {
     "duration": 0.020208,
     "end_time": "2021-12-16T12:00:01.918645",
     "exception": false,
     "start_time": "2021-12-16T12:00:01.898437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.text.values[idx]\n",
    "        \n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48bd9836",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T12:00:01.947869Z",
     "iopub.status.busy": "2021-12-16T12:00:01.947162Z",
     "iopub.status.idle": "2021-12-16T12:00:01.952724Z",
     "shell.execute_reply": "2021-12-16T12:00:01.953325Z",
     "shell.execute_reply.started": "2021-12-16T04:46:47.650187Z"
    },
    "papermill": {
     "duration": 0.022674,
     "end_time": "2021-12-16T12:00:01.953529",
     "exception": false,
     "start_time": "2021-12-16T12:00:01.930855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    input_ids, attention_mask = [], []\n",
    "    \n",
    "    tokenized_inputs = tokenizer(\n",
    "        data,\n",
    "        max_length=CFG['max_len'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    words = []\n",
    "    for i in range(len(data)):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        words.append(word_ids)\n",
    "\n",
    "    tokenized_inputs[\"word_ids\"] = words\n",
    "    \n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51129634",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T12:00:01.981633Z",
     "iopub.status.busy": "2021-12-16T12:00:01.981051Z",
     "iopub.status.idle": "2021-12-16T12:00:02.190281Z",
     "shell.execute_reply": "2021-12-16T12:00:02.189581Z",
     "shell.execute_reply.started": "2021-12-16T04:46:47.66221Z"
    },
    "papermill": {
     "duration": 0.224162,
     "end_time": "2021-12-16T12:00:02.190479",
     "exception": false,
     "start_time": "2021-12-16T12:00:01.966317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  1590,    10,  ...,  1843,  1901,     2],\n",
       "        [    0, 11102,  5717,  ...,     1,     1,     1],\n",
       "        [    0,  1812,   207,  ...,    33,    57,     2],\n",
       "        [    0,  6319,    47,  ...,     6,    24,     2],\n",
       "        [    0,   520,    82,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'word_ids': [[None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 15, 16, 16, 17, 18, 19, 20, 21, 22, 23, 24, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 40, 41, 41, 42, 43, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 64, 65, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 82, 82, 83, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 103, 103, 103, 104, 105, 106, 107, 107, 108, 109, 110, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 119, 120, 121, 122, 122, 123, 124, 125, 126, 127, 128, 129, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 143, 144, 145, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 157, 158, 159, 160, 161, 162, 163, 163, 164, 164, 165, 166, 167, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 176, 177, 178, 179, 180, 181, 182, 183, 184, 184, 184, 185, 186, 187, 187, 188, 189, 190, 191, 192, 192, 193, 194, 194, 195, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 229, 230, 230, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 243, 244, 244, 245, 246, 247, 248, 248, 249, 249, 250, 251, 252, 253, 254, 255, 256, 256, 257, 258, 259, 260, 261, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 270, 271, 272, 273, 274, 275, 275, 276, 277, 278, 279, 280, 281, 282, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 292, 293, 294, 295, 296, 297, 298, 299, 299, 300, 301, 302, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 313, 314, 314, 315, 316, 317, 318, 319, 320, 320, 320, 321, 322, 323, 324, 325, 326, 326, 327, 328, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 340, 341, 342, 342, 343, 344, 345, 346, 347, 348, 349, 350, 350, 351, 352, 353, 354, 354, 354, 354, 355, 356, 357, 358, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 371, 371, 372, 372, 373, 373, 373, 374, 375, 376, 377, 378, 379, 380, 380, 381, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 391, 392, 393, 394, 395, 396, 396, 396, 397, 398, 399, 400, 400, 401, 402, 403, 404, 404, 404, 404, 405, 405, 406, 407, 408, 408, 409, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 423, 424, 425, 426, 426, 426, 427, None], [None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 20, 21, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 66, 67, 68, 69, 70, 71, 72, 73, 73, 74, 75, 76, 77, 78, 79, 79, 80, 81, 82, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 206, 207, 208, 208, 209, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 241, 242, 243, 244, 245, 246, 247, 248, 249, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 313, 314, 315, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 361, 362, 363, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 377, 378, 379, 380, 381, 382, 383, 384, 385, 385, 386, 386, 387, 388, 388, 389, 390, 391, 392, 393, 393, 393, 394, 395, 396, 397, 398, 399, 400, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 420, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 12, 13, 14, 15, 16, 16, 17, 18, 19, 20, 21, 22, 23, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 38, 39, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 53, 54, 54, 55, 56, 56, 57, 57, 58, 59, 60, 61, 62, 63, 64, 64, 64, 65, 66, 67, 68, 69, 70, 70, 71, 72, 73, 74, 75, 76, 77, 77, 78, 79, 79, 80, 81, 82, 83, 84, 85, 85, 85, 86, 87, 88, 89, 90, 91, 92, 92, 93, 94, 95, 96, 97, 97, 98, 99, 100, 101, 102, 103, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 113, 114, 115, 115, 116, 117, 118, 119, 120, 121, 122, 123, 123, 124, 124, 125, 126, 127, 128, 129, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 145, 146, 147, 148, 149, 150, 151, 152, 153, 153, 154, 155, 156, 157, 158, 159, 160, 161, 161, 162, 163, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 172, 172, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 188, 189, 190, 191, 192, 192, 193, 194, 194, 194, 195, 196, 197, 198, 199, 200, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 209, 209, 210, 211, 211, 211, 211, 212, 212, 213, 214, 215, 216, 217, 217, 217, 218, 218, 219, 220, 221, 222, 223, 224, 225, 226, 226, 227, 228, 229, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 244, 245, 246, 247, 248, 249, 249, 250, 251, 252, 252, 252, 252, 253, 254, 255, 256, 256, 256, 256, 257, 257, 258, 259, 260, 261, 262, 263, 263, 264, 265, 266, 267, 268, 269, 270, 271, 271, 272, 273, 273, 273, 273, 274, 275, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 290, 291, 292, 293, 293, 294, 295, 296, 297, 298, 299, 300, 300, 300, 300, 301, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 322, 322, 322, 323, 324, 325, 326, 327, 328, 329, 329, 330, 330, 330, 331, 332, 332, 333, 334, 335, 336, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 345, 346, 346, 347, 348, 348, 349, 350, 351, 352, 353, 354, 355, 356, 356, 357, 358, 359, 359, 359, 359, 360, 361, 362, 363, 364, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 375, 376, 377, 377, 378, 379, 380, 380, 381, 381, 381, 382, 383, 384, 385, 386, 387, 388, 388, 389, 390, 390, 390, 390, 391, 392, 392, 393, 394, 395, 396, 397, 398, 399, 400, 400, 400, 401, 402, 403, 404, 404, 405, 406, 407, 407, 408, 409, None], [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 17, 18, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 28, 29, 29, 30, 31, 32, 33, 34, 34, 35, 36, 37, 37, 38, 39, 40, 41, 42, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 90, 91, 92, 93, 94, 95, 96, 97, 98, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 107, 108, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 144, 145, 146, 147, 148, 149, 149, 150, 151, 151, 152, 153, 154, 155, 156, 157, 158, 158, 159, 160, 161, 162, 163, 164, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 179, 180, 181, 182, 183, 184, 185, 186, 187, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 201, 202, 203, 204, 205, 206, 207, 208, 209, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 219, 220, 221, 222, 223, 224, 225, 226, 226, 227, 228, 229, 230, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 254, 255, 256, 257, 258, 259, 260, 261, 262, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 300, 301, 301, 302, 303, 304, 305, 306, 307, 308, 309, 309, 310, 311, 312, 313, 314, 315, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 380, 381, 382, 383, 384, 385, 386, 387, 388, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 397, 398, 399, 400, 401, 402, 403, 404, 404, 405, 406, 407, 408, 408, 409, 410, 411, 412, 413, 414, 415, 415, 416, 417, 418, 419, 420, 421, 422, 423, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 449, 450, 451, 452, 453, 454, 455, 456, 456, 457, 457, 458, None], [None, 0, 1, 2, 3, 4, 4, 4, 5, 6, 7, 8, 9, 10, 11, 11, 12, 13, 14, 15, 15, 15, 15, 16, 17, 18, 19, 19, 20, 21, 22, 23, 24, 25, 26, 27, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 38, 39, 40, 41, 42, 42, 43, 44, 45, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 65, 66, 67, 68, 69, 70, 71, 72, 73, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 101, 102, 103, 104, 105, 106, 107, 108, 108, 109, 110, 111, 112, 113, 114, 114, 115, 116, 117, 118, 118, 118, 119, 120, 121, 121, 122, 123, 124, 125, 126, 127, 127, 128, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 138, 139, 140, 141, 142, 143, 143, 144, 145, 146, 147, 148, 149, 149, 150, 151, 152, 153, 154, 155, 156, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 170, 171, 172, 173, 174, 175, 176, 177, 178, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 195, 196, 197, 198, 199, 200, 201, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 222, 223, 224, 225, 226, 227, 228, 229, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 238, 239, 240, 241, 241, 241, 242, 242, 243, 244, 245, 245, 246, 247, 248, 249, 250, 251, 252, 252, 253, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 267, 268, 269, 270, 271, 271, 272, 273, 274, 275, 276, 277, 278, 278, 279, 280, 281, 282, 283, 284, 285, 285, 286, 287, 287, 288, 289, 290, 291, 292, 293, 294, 295, 295, 296, 297, 298, 298, 298, 299, 300, 301, 302, 302, 303, 304, 305, 305, 306, 306, 307, 308, 309, 310, 311, 312, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 323, 324, 324, 325, 326, 327, 328, 328, 329, 330, 331, 332, 332, 333, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 343, 343, 344, 345, 346, 347, 348, 349, 349, 350, 351, 351, 352, 353, 354, 355, 355, 356, 356, 357, 357, 358, 359, 360, 361, 362, 362, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader = DataLoader(MyDataset(test_texts), batch_size=CFG['valid_bs'], collate_fn=collate_fn, shuffle=False, num_workers=4)\n",
    "batch = next(iter(test_loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a9fc8a9",
   "metadata": {
    "_kg_hide-output": true,
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-12-16T12:00:02.222319Z",
     "iopub.status.busy": "2021-12-16T12:00:02.221629Z",
     "iopub.status.idle": "2021-12-16T12:00:12.813204Z",
     "shell.execute_reply": "2021-12-16T12:00:12.813635Z",
     "shell.execute_reply.started": "2021-12-16T04:46:47.870847Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 10.61037,
     "end_time": "2021-12-16T12:00:12.813849",
     "exception": false,
     "start_time": "2021-12-16T12:00:02.203479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/roberta-base were not used when initializing RobertaForTokenClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at ../input/roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForTokenClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=15, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model =  AutoModelForTokenClassification.from_pretrained(CFG['model'], num_labels=15).to(device)\n",
    "model.load_state_dict(torch.load('../input/feedback-roberta/roberta-base_fold_0.pt', map_location=torch.device('cpu')))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2459ce6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T12:00:12.850732Z",
     "iopub.status.busy": "2021-12-16T12:00:12.845153Z",
     "iopub.status.idle": "2021-12-16T12:00:26.280233Z",
     "shell.execute_reply": "2021-12-16T12:00:26.282574Z",
     "shell.execute_reply.started": "2021-12-16T04:47:06.307549Z"
    },
    "papermill": {
     "duration": 13.456159,
     "end_time": "2021-12-16T12:00:26.283784",
     "exception": false,
     "start_time": "2021-12-16T12:00:12.827625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:13<00:00, 13.41s/it]\n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "words = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    tk = tqdm(test_loader, total=len(test_loader), position=0, leave=True)\n",
    "    for step, batch in enumerate(tk):\n",
    "        word_ids = batch['word_ids']\n",
    "        words.extend(word_ids)\n",
    "        batch = {k: v.to(device) for k, v in batch.items() if k != 'word_ids'}\n",
    "\n",
    "        output = model(**batch).logits\n",
    "\n",
    "        y_pred.extend(output.argmax(-1).cpu().numpy())\n",
    "        \n",
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1307339d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T12:00:26.398753Z",
     "iopub.status.busy": "2021-12-16T12:00:26.394384Z",
     "iopub.status.idle": "2021-12-16T12:00:26.417430Z",
     "shell.execute_reply": "2021-12-16T12:00:26.413759Z",
     "shell.execute_reply.started": "2021-12-16T04:47:07.862634Z"
    },
    "papermill": {
     "duration": 0.096643,
     "end_time": "2021-12-16T12:00:26.418103",
     "exception": false,
     "start_time": "2021-12-16T12:00:26.321460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = ['O', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position', 'B-Claim', 'I-Claim', 'B-Counterclaim', 'I-Counterclaim', \n",
    "          'B-Rebuttal', 'I-Rebuttal', 'B-Evidence', 'I-Evidence', 'B-Concluding Statement', 'I-Concluding Statement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b648d6e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T12:00:26.467608Z",
     "iopub.status.busy": "2021-12-16T12:00:26.466736Z",
     "iopub.status.idle": "2021-12-16T12:00:26.496294Z",
     "shell.execute_reply": "2021-12-16T12:00:26.497012Z",
     "shell.execute_reply.started": "2021-12-16T04:48:19.005107Z"
    },
    "papermill": {
     "duration": 0.053146,
     "end_time": "2021-12-16T12:00:26.497261",
     "exception": false,
     "start_time": "2021-12-16T12:00:26.444115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 518.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('0FB0700DAF44',\n",
       " 'Lead',\n",
       " '0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_preds = []\n",
    "\n",
    "for i in tqdm(range(len(test_texts))):\n",
    "    idx = test_texts.id.values[i]\n",
    "    pred = ['']*len(test_texts.text.values[i])\n",
    "\n",
    "    for j in range(len(y_pred[i])):\n",
    "        if words[i][j] != None:\n",
    "            pred[words[i][j]] = labels[y_pred[i][j]]\n",
    "\n",
    "    pred = [x.replace('B-','').replace('I-','') for x in pred]\n",
    "\n",
    "    preds = []\n",
    "    j = 0\n",
    "    while j < len(pred):\n",
    "        cls = pred[j]\n",
    "        if cls == 'O':\n",
    "            j += 1\n",
    "        end = j + 1\n",
    "        while end < len(pred) and pred[end] == cls:\n",
    "            end += 1\n",
    "            \n",
    "        if cls != 'O' and cls != '' and end - j > 10:\n",
    "            final_preds.append((idx, cls, ' '.join(map(str, list(range(j, end))))))\n",
    "        \n",
    "        j = end\n",
    "        \n",
    "final_preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "914b0a88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T12:00:26.538428Z",
     "iopub.status.busy": "2021-12-16T12:00:26.537687Z",
     "iopub.status.idle": "2021-12-16T12:00:26.564862Z",
     "shell.execute_reply": "2021-12-16T12:00:26.564079Z",
     "shell.execute_reply.started": "2021-12-16T04:48:21.249461Z"
    },
    "papermill": {
     "duration": 0.048637,
     "end_time": "2021-12-16T12:00:26.565062",
     "exception": false,
     "start_time": "2021-12-16T12:00:26.516425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "      <th>predictionstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>Lead</td>\n",
       "      <td>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>Claim</td>\n",
       "      <td>49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>Claim</td>\n",
       "      <td>66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>Position</td>\n",
       "      <td>108 109 110 111 112 113 114 115 116 117 118 119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>123 124 125 126 127 128 129 130 131 132 133 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>Claim</td>\n",
       "      <td>314 315 316 317 318 319 320 321 322 323 324 32...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>341 342 343 344 345 346 347 348 349 350 351 35...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>D72CB1C11673</td>\n",
       "      <td>Lead</td>\n",
       "      <td>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>D72CB1C11673</td>\n",
       "      <td>Position</td>\n",
       "      <td>50 51 52 53 54 55 56 57 58 59 60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>D72CB1C11673</td>\n",
       "      <td>Claim</td>\n",
       "      <td>63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>D72CB1C11673</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>94 95 96 97 98 99 100 101 102 103 104 105 106 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>D72CB1C11673</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>183 184 185 186 187 188 189 190 191 192 193 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>D72CB1C11673</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>274 275 276 277 278 279 280 281 282 283 284 28...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>D72CB1C11673</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>289 290 291 292 293 294 295 296 297 298 299 30...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>D72CB1C11673</td>\n",
       "      <td>Concluding Statement</td>\n",
       "      <td>364 365 366 367 368 369 370 371 372 373 374 37...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>18409261F5C2</td>\n",
       "      <td>Position</td>\n",
       "      <td>0 1 2 3 4 5 6 7 8 9 10 11 12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18409261F5C2</td>\n",
       "      <td>Claim</td>\n",
       "      <td>18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18409261F5C2</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18409261F5C2</td>\n",
       "      <td>Lead</td>\n",
       "      <td>71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>18409261F5C2</td>\n",
       "      <td>Position</td>\n",
       "      <td>124 125 126 127 128 129 130 131 132 133 134 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>18409261F5C2</td>\n",
       "      <td>Claim</td>\n",
       "      <td>138 139 140 141 142 143 144 145 146 147 148 14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>18409261F5C2</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>162 163 164 165 166 167 168 169 170 171 172 17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>DF920E0A7337</td>\n",
       "      <td>Lead</td>\n",
       "      <td>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>DF920E0A7337</td>\n",
       "      <td>Position</td>\n",
       "      <td>68 69 70 71 72 73 74 75 76 77 78 79 80 81 82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>DF920E0A7337</td>\n",
       "      <td>Claim</td>\n",
       "      <td>85 86 87 88 89 90 91 92 93 94 95 96 97 98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>DF920E0A7337</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>118 119 120 121 122 123 124 125 126 127 128 12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>DF920E0A7337</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>310 311 312 313 314 315 316 317 318 319 320 32...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>D46BCB48440A</td>\n",
       "      <td>Lead</td>\n",
       "      <td>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>D46BCB48440A</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>D46BCB48440A</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>150 151 152 153 154 155 156 157 158 159 160 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>D46BCB48440A</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>223 224 225 226 227 228 229 230 231 232 233 23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>D46BCB48440A</td>\n",
       "      <td>Concluding Statement</td>\n",
       "      <td>306 307 308 309 310 311 312 313 314 315 316 31...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                 class  \\\n",
       "0   0FB0700DAF44                  Lead   \n",
       "1   0FB0700DAF44                 Claim   \n",
       "2   0FB0700DAF44                 Claim   \n",
       "3   0FB0700DAF44              Position   \n",
       "4   0FB0700DAF44              Evidence   \n",
       "5   0FB0700DAF44                 Claim   \n",
       "6   0FB0700DAF44              Evidence   \n",
       "7   D72CB1C11673                  Lead   \n",
       "8   D72CB1C11673              Position   \n",
       "9   D72CB1C11673                 Claim   \n",
       "10  D72CB1C11673              Evidence   \n",
       "11  D72CB1C11673              Evidence   \n",
       "12  D72CB1C11673              Evidence   \n",
       "13  D72CB1C11673              Evidence   \n",
       "14  D72CB1C11673  Concluding Statement   \n",
       "15  18409261F5C2              Position   \n",
       "16  18409261F5C2                 Claim   \n",
       "17  18409261F5C2              Evidence   \n",
       "18  18409261F5C2                  Lead   \n",
       "19  18409261F5C2              Position   \n",
       "20  18409261F5C2                 Claim   \n",
       "21  18409261F5C2              Evidence   \n",
       "22  DF920E0A7337                  Lead   \n",
       "23  DF920E0A7337              Position   \n",
       "24  DF920E0A7337                 Claim   \n",
       "25  DF920E0A7337              Evidence   \n",
       "26  DF920E0A7337              Evidence   \n",
       "27  D46BCB48440A                  Lead   \n",
       "28  D46BCB48440A              Evidence   \n",
       "29  D46BCB48440A              Evidence   \n",
       "30  D46BCB48440A              Evidence   \n",
       "31  D46BCB48440A  Concluding Statement   \n",
       "\n",
       "                                     predictionstring  \n",
       "0   0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...  \n",
       "1     49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64  \n",
       "2   66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 8...  \n",
       "3     108 109 110 111 112 113 114 115 116 117 118 119  \n",
       "4   123 124 125 126 127 128 129 130 131 132 133 13...  \n",
       "5   314 315 316 317 318 319 320 321 322 323 324 32...  \n",
       "6   341 342 343 344 345 346 347 348 349 350 351 35...  \n",
       "7   0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...  \n",
       "8                    50 51 52 53 54 55 56 57 58 59 60  \n",
       "9   63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 7...  \n",
       "10  94 95 96 97 98 99 100 101 102 103 104 105 106 ...  \n",
       "11  183 184 185 186 187 188 189 190 191 192 193 19...  \n",
       "12  274 275 276 277 278 279 280 281 282 283 284 28...  \n",
       "13  289 290 291 292 293 294 295 296 297 298 299 30...  \n",
       "14  364 365 366 367 368 369 370 371 372 373 374 37...  \n",
       "15                       0 1 2 3 4 5 6 7 8 9 10 11 12  \n",
       "16  18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 3...  \n",
       "17    55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70  \n",
       "18  71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 8...  \n",
       "19  124 125 126 127 128 129 130 131 132 133 134 13...  \n",
       "20  138 139 140 141 142 143 144 145 146 147 148 14...  \n",
       "21  162 163 164 165 166 167 168 169 170 171 172 17...  \n",
       "22  0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...  \n",
       "23       68 69 70 71 72 73 74 75 76 77 78 79 80 81 82  \n",
       "24          85 86 87 88 89 90 91 92 93 94 95 96 97 98  \n",
       "25  118 119 120 121 122 123 124 125 126 127 128 12...  \n",
       "26  310 311 312 313 314 315 316 317 318 319 320 32...  \n",
       "27  0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  \n",
       "28  56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 7...  \n",
       "29  150 151 152 153 154 155 156 157 158 159 160 16...  \n",
       "30  223 224 225 226 227 228 229 230 231 232 233 23...  \n",
       "31  306 307 308 309 310 311 312 313 314 315 316 31...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.DataFrame(final_preds)\n",
    "sub.columns = test_df.columns\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47d7e5e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T12:00:26.608876Z",
     "iopub.status.busy": "2021-12-16T12:00:26.607951Z",
     "iopub.status.idle": "2021-12-16T12:00:26.621119Z",
     "shell.execute_reply": "2021-12-16T12:00:26.621681Z",
     "shell.execute_reply.started": "2021-12-16T04:47:07.928367Z"
    },
    "papermill": {
     "duration": 0.03739,
     "end_time": "2021-12-16T12:00:26.621881",
     "exception": false,
     "start_time": "2021-12-16T12:00:26.584491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 50.728443,
   "end_time": "2021-12-16T12:00:30.186976",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-12-16T11:59:39.458533",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
