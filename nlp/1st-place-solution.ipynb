{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Google Quest Q&A Labeling\n",
    "## <center> 1st place solution\n",
    "#### <center> by Dmitriy Danevskiy, Oleg Yaroshevskiy, Yury Kashnitsky, and Dmitriy Abulkhanov\n",
    "\n",
    "The purpose of this competition is to analyze StackExchange questions & answers predicting whether the question is interesting, whether the answer is helpful or misleading etc. So in theory, top solutions can help Q&A systems in getting more human-like.\n",
    "\n",
    "\n",
    "In a nutshell, our team trained 4 models: 2 [BERT](https://arxiv.org/abs/1810.04805) ones, one [RoBERTa](https://arxiv.org/abs/1907.11692), and one [BART](https://arxiv.org/abs/1910.13461). Key ideas are:\n",
    "- pretraining language models with StackExchange data and auxiliary targets\n",
    "- pseudo-labeling\n",
    "- postprocessing predictions\n",
    "\n",
    "Details are outlined [in this post](https://www.kaggle.com/c/google-quest-challenge/discussion/129840), code is shared in [this repository](https://github.com/oleg-yaroshevskiy/quest_qa_labeling). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install necessary packages**\n",
    " - [mag](https://github.com/ex4sperans/mag) is a lightweight library to keep track of experiments\n",
    " - sacremoses is a dependency for transformers\n",
    " - sacreBLEU and fairseq are dependencies for the BART model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.08 s, sys: 526 ms, total: 2.6 s\n",
      "Wall time: 2min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!pip install /kaggle/input/pythonmag/mag > /dev/null\n",
    "!pip install ../input/sacremoses/sacremoses-master/ > /dev/null\n",
    "!pip install /kaggle/input/sacrebleu/sacreBLEU-master/ > /dev/null\n",
    "!pip install /kaggle/input/fairseq-hacked/fairseq > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Inference\n",
    "\n",
    "### Model 1. BERT base uncased\n",
    "\n",
    "This is an uncased BERT model, its LM is finetuned with StackExchange data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-02 23:17:04.974678: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n",
      "100%|███████████████████████████████████████████| 60/60 [00:10<00:00,  5.98it/s]\r\n",
      "100%|███████████████████████████████████████████| 60/60 [00:09<00:00,  6.43it/s]\r\n",
      "100%|███████████████████████████████████████████| 60/60 [00:09<00:00,  6.38it/s]\r\n",
      "100%|███████████████████████████████████████████| 60/60 [00:09<00:00,  6.44it/s]\r\n",
      "100%|███████████████████████████████████████████| 60/60 [00:09<00:00,  6.21it/s]\r\n",
      "CPU times: user 1.83 s, sys: 436 ms, total: 2.26 s\n",
      "Wall time: 1min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python /kaggle/input/old-bert-code/predict_test.py \\\n",
    "  --model_dir /kaggle/input/stackx-80-aux-ep-3       \\\n",
    "  --sub_file model1_bert_base_uncased_pred.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2. BERT base cased\n",
    "\n",
    "This is a cased BERT model, its LM is finetuned with StackExchange data, code has been refactored w.r.t. to the first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-02 23:18:59.025886: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n",
      "Initial arguments Namespace(batch_accumulation=4, batch_size=8, bert_model='/kaggle/input/bert-base-pretrained/stackx-base-cased/', data_path='/kaggle/input/google-quest-challenge/', epochs=5, folds=5, head_tail=True, label='qa', leak_free_pseudo=False, lr=2e-05, max_answer_length=210, max_question_length=260, max_sequence_length=500, max_title_length=26, num_classes=30, pseudo_file=None, seed=42, split_pseudo=False, sub_file='model2_bert_base_cased_pred.csv', warmup=200, workers=8)\r\n",
      "Preparing dataset: 100%|█████████████████████| 476/476 [00:03<00:00, 122.68it/s]\r\n",
      "Fold 0\r\n",
      "Test: 100%|█████████████████████████████████████| 60/60 [00:09<00:00,  6.62it/s]\r\n",
      "Fold 1\r\n",
      "Test: 100%|█████████████████████████████████████| 60/60 [00:08<00:00,  6.75it/s]\r\n",
      "Fold 2\r\n",
      "Test: 100%|█████████████████████████████████████| 60/60 [00:08<00:00,  6.76it/s]\r\n",
      "Fold 3\r\n",
      "Test: 100%|█████████████████████████████████████| 60/60 [00:08<00:00,  6.76it/s]\r\n",
      "Fold 4\r\n",
      "Test: 100%|█████████████████████████████████████| 60/60 [00:08<00:00,  6.72it/s]\r\n",
      "CPU times: user 1.58 s, sys: 336 ms, total: 1.92 s\n",
      "Wall time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python ../input/bert-base-random-code/run.py                \\\n",
    "  --sub_file=model2_bert_base_cased_pred.csv                  \\\n",
    "  --data_path=/kaggle/input/google-quest-challenge/            \\\n",
    "  --max_sequence_length=500                                     \\\n",
    "  --max_title_length=26                                          \\\n",
    "  --max_question_length=260                                       \\\n",
    "  --max_answer_length=210                                          \\\n",
    "  --batch_size=8                                                    \\\n",
    "  --bert_model=/kaggle/input/bert-base-pretrained/stackx-base-cased/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3. RoBERTa\n",
    "\n",
    "Here we're resorting to both LM finetuning and pseudo-labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setups\n",
    "\n",
    "ROBERTA_EXPERIMENT_DIR = \"2-4-roberta-base-saved-5-head_tail-roberta-stackx-base-v2-pl1kksample20k-1e-05-210-260-500-26-roberta-200\"\n",
    "!mkdir $ROBERTA_EXPERIMENT_DIR\n",
    "!ln -s /kaggle/input/roberta-stackx-base-pl20k/checkpoints $ROBERTA_EXPERIMENT_DIR/checkpoints\n",
    "\n",
    "ROBERTA_CONFIG = {\n",
    "    \"_seed\": 42,\n",
    "    \"batch_accumulation\": 2,\n",
    "    \"batch_size\": 4,\n",
    "    \"bert_model\": \"roberta-base-saved\",\n",
    "    \"folds\": 5,\n",
    "    \"head_tail\": True,\n",
    "    \"label\": \"roberta-stackx-base-v2-pl1kksample20k\",\n",
    "    \"lr\": 1e-05,\n",
    "    \"max_answer_length\": 210,\n",
    "    \"max_question_length\": 260,\n",
    "    \"max_sequence_length\": 500,\n",
    "    \"max_title_length\": 26,\n",
    "    \"model_type\": \"roberta\",\n",
    "    \"warmup\": 200\n",
    "}\n",
    "with open(os.path.join(ROBERTA_EXPERIMENT_DIR, \"config.json\"), \"w\") as fp:\n",
    "    json.dump(ROBERTA_CONFIG, fp)\n",
    "    \n",
    "!echo kek > $ROBERTA_EXPERIMENT_DIR/command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-02 23:20:35.414736: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n",
      "\r\n",
      "Fold: 0\r\n",
      "\r\n",
      "Test: 100%|███████████████████████████████████| 119/119 [00:12<00:00,  9.35it/s]\r\n",
      "\r\n",
      "Fold: 1\r\n",
      "\r\n",
      "Test: 100%|███████████████████████████████████| 119/119 [00:11<00:00, 10.44it/s]\r\n",
      "\r\n",
      "Fold: 2\r\n",
      "\r\n",
      "Test: 100%|███████████████████████████████████| 119/119 [00:11<00:00, 10.49it/s]\r\n",
      "\r\n",
      "Fold: 3\r\n",
      "\r\n",
      "Test: 100%|███████████████████████████████████| 119/119 [00:11<00:00, 10.35it/s]\r\n",
      "\r\n",
      "Fold: 4\r\n",
      "\r\n",
      "Test: 100%|███████████████████████████████████| 119/119 [00:11<00:00, 10.49it/s]\r\n",
      "CPU times: user 1.87 s, sys: 360 ms, total: 2.23 s\n",
      "Wall time: 1min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python ../input/roberta-base-code/infer.py                 \\\n",
    "  --experiment $ROBERTA_EXPERIMENT_DIR                       \\\n",
    "  --checkpoint=best_model.pth                                 \\\n",
    "  --bert_model=/kaggle/input/roberta-base-model                \\\n",
    "  --dataframe=/kaggle/input/google-quest-challenge/test.csv     \\\n",
    "  --output_dir=roberta-base-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4. BART\n",
    "\n",
    "BART-large, with pseudo-labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial arguments Namespace(batch_accumulation=4, batch_size=4, bert_model='bart.large', data_path='/kaggle/input/google-quest-challenge/', epochs=5, folds=5, head_tail=True, label='qa', leak_free_pseudo=False, lr=2e-05, max_answer_length=210, max_question_length=260, max_sequence_length=500, max_title_length=26, num_classes=30, pseudo_file=None, seed=42, split_pseudo=False, sub_file='model4_bart_large_pred.csv', warmup=200, workers=8)\r\n",
      "| dictionary: 50264 types\r\n",
      "Almost done\r\n",
      "Preparing dataset: 100%|██████████████████████| 476/476 [00:04<00:00, 99.82it/s]\r\n",
      "| dictionary: 50264 types\r\n",
      "Almost done\r\n",
      "Fold 0\r\n",
      "Test: 100%|███████████████████████████████████| 119/119 [00:37<00:00,  3.21it/s]\r\n",
      "Fold 1\r\n",
      "Test: 100%|███████████████████████████████████| 119/119 [00:36<00:00,  3.22it/s]\r\n",
      "Fold 2\r\n",
      "Test: 100%|███████████████████████████████████| 119/119 [00:36<00:00,  3.22it/s]\r\n",
      "Fold 3\r\n",
      "Test: 100%|███████████████████████████████████| 119/119 [00:36<00:00,  3.22it/s]\r\n",
      "Fold 4\r\n",
      "Test: 100%|███████████████████████████████████| 119/119 [00:36<00:00,  3.22it/s]\r\n",
      "CPU times: user 4.58 s, sys: 1.05 s, total: 5.64 s\n",
      "Wall time: 5min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python ../input/bart-code/run.py                      \\\n",
    "  --sub_file=model4_bart_large_pred.csv                 \\\n",
    "  --data_path=/kaggle/input/google-quest-challenge/      \\\n",
    "  --max_sequence_length=500                               \\\n",
    "  --max_title_length=26                                    \\\n",
    "  --max_question_length=260                                 \\\n",
    "  --max_answer_length=210                                    \\\n",
    "  --batch_size=4                                              \\\n",
    "  --bert_model=bart.large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blending and postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First, we read the 30 target columns that we need to predict.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 30 targets to predict\n"
     ]
    }
   ],
   "source": [
    "sample_submission_df = pd.read_csv(\"/kaggle/input/google-quest-challenge/sample_submission.csv\", \n",
    "                             index_col='qa_id')\n",
    "target_columns = sample_submission_df.columns\n",
    "print(f'There are {len(target_columns)} targets to predict')\n",
    "\n",
    "train_df = pd.read_csv(\"/kaggle/input/google-quest-challenge/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading submission files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_bert_base_uncased_pred_df = pd.read_csv(\"model1_bert_base_uncased_pred.csv\")\n",
    "model2_bert_base_cased_pred_df = pd.read_csv(\"model2_bert_base_cased_pred.csv\")\n",
    "model4_bart_large_pred_df = pd.read_csv(\"model4_bart_large_pred.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For RoBERTa, we average predictions from 5 folds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_base_dfs = [pd.read_csv(\n",
    "                    os.path.join(\"roberta-base-output\", \"fold-{}.csv\".format(fold))) \n",
    "                    for fold in range(5)]\n",
    "\n",
    "model3_roberta_pred_df = roberta_base_dfs[0].copy()\n",
    "\n",
    "for col in target_columns:\n",
    "    model3_roberta_pred_df[col] = np.mean([df[col] for df in roberta_base_dfs], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Blending**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "blended_df = model3_roberta_pred_df.copy()\n",
    "\n",
    "for col in target_columns:\n",
    "    blended_df[col] = (\n",
    "        model1_bert_base_uncased_pred_df[col] * 0.1 +\n",
    "        model2_bert_base_cased_pred_df[col] * 0.2 + \n",
    "        model3_roberta_pred_df[col] * 0.1 + \n",
    "        model4_bart_large_pred_df[col] * 0.3\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applying postprocessing to the final blend, also discussed [here](https://www.kaggle.com/c/google-quest-challenge/discussion/129840).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_single(target, ref):\n",
    "    \"\"\"\n",
    "    The idea here is to make the distribution of a particular predicted column\n",
    "    to match the correspoding distribution of the corresponding column in the\n",
    "    training dataset (called ref here)\n",
    "    \"\"\"\n",
    "    \n",
    "    ids = np.argsort(target)\n",
    "    counts = sorted(Counter(ref).items(), key=lambda s: s[0])\n",
    "    scores = np.zeros_like(target)\n",
    "    \n",
    "    last_pos = 0\n",
    "    v = 0\n",
    "    \n",
    "    for value, count in counts:\n",
    "        next_pos = last_pos + int(round(count / len(ref) * len(target)))\n",
    "        if next_pos == last_pos:\n",
    "            next_pos += 1\n",
    "\n",
    "        cond = ids[last_pos:next_pos]\n",
    "        scores[cond] = v\n",
    "        last_pos = next_pos\n",
    "        v += 1\n",
    "        \n",
    "    return scores / scores.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_prediction(prediction, actual):\n",
    "    \n",
    "    postprocessed = prediction.copy()\n",
    "    \n",
    "    for col in target_columns:\n",
    "        scores = postprocess_single(prediction[col].values, actual[col].values)\n",
    "        # Those are columns where our postprocessing gave substantial improvement.\n",
    "        # It also helped for some others, but we didn't include them as the gain was\n",
    "        # very marginal (less than 0.01)\n",
    "        if col in (\n",
    "            \"question_conversational\",\n",
    "            \"question_type_compare\",\n",
    "            \"question_type_definition\",\n",
    "            \"question_type_entity\",\n",
    "            \"question_has_commonly_accepted_answer\",\n",
    "            \"question_type_consequence\",\n",
    "            \"question_type_spelling\"\n",
    "        ):\n",
    "            postprocessed[col] = scores\n",
    "            \n",
    "        # scale to 0-1 interval\n",
    "        v = postprocessed[col].values\n",
    "        postprocessed[col] = (v - v.min()) / (v.max() - v.min())\n",
    "    \n",
    "    return postprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocessed = postprocess_prediction(blended_df, train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving the submission file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocessed.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
