{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Welcome and have fun learning Logistic Regession with plug and play Neural Network\n\n#### You are dealing with a **heavly skewed** dataset when some classes are much more frequent than others. **Accuracy** is not the perferred performance measure for classifiers.\n\n<blockquote style=\"margin-right:auto; margin-left:auto; padding: 1em; margin:24px;\">\n    <strong>Fork This Notebook!</strong><br>\nCreate your own editable copy of this notebook by clicking on the <strong>Copy and Edit</strong> button in the top right corner.\n</blockquote>\n\n**Notes:**\nRun time -\n4 hours and 31 minutes 4000000 samples\n3318.3s 395712 samples\n4375.7s 1468136 samples\n12905.1s 2262087 samples\n5555.3s - GPU 2262087 samples\n\n## Imports and Configuration ##","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nfrom scipy import stats\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\nimport seaborn as sns\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler, TomekLinks\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, RobustScaler, PowerTransformer\nle = LabelEncoder()\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.metrics import accuracy_score, log_loss, confusion_matrix, roc_curve, precision_recall_curve\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\nimport tensorflow.keras as keras\n# import keras\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport gc\nimport os\nimport math\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.038299,"end_time":"2021-11-20T07:24:02.02488","exception":false,"start_time":"2021-11-20T07:24:01.986581","status":"completed"},"tags":[],"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-08T03:07:43.217153Z","iopub.execute_input":"2021-12-08T03:07:43.217497Z","iopub.status.idle":"2021-12-08T03:07:50.661429Z","shell.execute_reply.started":"2021-12-08T03:07:43.217407Z","shell.execute_reply":"2021-12-08T03:07:50.660444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -----------------------------------------------------------------\n# Some parameters to config \n\nEPOCHS = 177      # Does not matter with Early stopping.\nBATCH_SIZE = 2048 # large enough to fit RAM\nACTIVATION = 'swish' # swish relu selu ;swish seem to work better with dropout layer\nFOLDS = 5        # Only 5 or 10.\nLEARNING_RATE = 0.000965713 # Optimal lr is about half the maximum lr\nLR_FACTOR = 0.5   # LEARNING_RATE * LR_FACTOR = New Learning rate on ReduceLROnPlateau\nRLRP_PATIENCE = 5 # Learning Rate reduction on ReduceLROnPlateau\nES_PATIENCE = 21  # Early stopping\nDROPOUT = 0.1     # Act like L1 L2 regulator.\n\nOPTIMIZER= 'adam' #adam adamax\nLOSS='sparse_categorical_crossentropy' # sparse_categorical_crossentropy categorical_crossentropy\nMETRICS='sparse_categorical_accuracy'  # accuracy categorical_accuracy\nACC_VAL_METRICS = 'val_sparse_categorical_accuracy' # 'val_acc' val_sparse_categorical_accuracy\nACC_METRICS = 'sparse_categorical_accuracy' # acc accuracy 'sparse_categorical_accuracy'\n\n# The dataset is too huge for trial. Sampling it for speed run!\nSAMPLE = 2262087  # For quick test: y counts [1468136, 2262087, 195712, 377, 1, 11426, 62261]  # 4000000 total rows\nVALIDATION_SPLIT = 0.15 # Only used to min dataset for quick test\nMAX_TRIAL = 3     # speed trial any% Not used here\n\nRANDOM_STATE = 42\nVERBOSE = 0\n\n# Admin\nID = \"Id\"         # Id id x X index\nINPUT = \"../input/tabular-playground-series-dec-2021\"","metadata":{"papermill":{"duration":0.023971,"end_time":"2021-11-20T07:24:02.06631","exception":false,"start_time":"2021-11-20T07:24:02.042339","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-08T03:07:50.664356Z","iopub.execute_input":"2021-12-08T03:07:50.665215Z","iopub.status.idle":"2021-12-08T03:07:50.672281Z","shell.execute_reply.started":"2021-12-08T03:07:50.665174Z","shell.execute_reply":"2021-12-08T03:07:50.671423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing ##\n\nBefore we can do any feature engineering, we need to *preprocess* the data to get it in a form suitable for analysis. We'll need to:\n- **Load** the data from CSV files\n- **Clean** the data to fix any errors or inconsistencies\n- **Encode** the statistical data type (numeric, categorical)\n- **Impute** any missing values\n\nWe'll wrap all these steps up in a function, which will make easy for you to get a fresh dataframe whenever you need. After reading the CSV file, we'll apply three preprocessing steps, `clean`, `encode`, and `impute`, and then create the data splits: one (`df_train`) for training the model, and one (`df_test`) for making the predictions that you'll submit to the competition for scoring on the leaderboard.","metadata":{"papermill":{"duration":0.016203,"end_time":"2021-11-20T07:24:02.101208","exception":false,"start_time":"2021-11-20T07:24:02.085005","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Handle Missing Values ###\n\nHandling missing values now will make the feature engineering go more smoothly. We'll impute `0` for missing numeric values and `\"None\"` for missing categorical values. You might like to experiment with other imputation strategies. In particular, you could try creating \"missing value\" indicators: `1` whenever a value was imputed and `0` otherwise.","metadata":{}},{"cell_type":"code","source":"def impute(df):\n    for name in df.select_dtypes(\"number\"):\n        df[name] = df[name].fillna(0)\n    for name in df.select_dtypes(\"category\"):\n        df[name] = df[name].fillna(\"None\")\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:07:50.673597Z","iopub.execute_input":"2021-12-08T03:07:50.674137Z","iopub.status.idle":"2021-12-08T03:07:50.697916Z","shell.execute_reply.started":"2021-12-08T03:07:50.674095Z","shell.execute_reply":"2021-12-08T03:07:50.696971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reduce Memory usage","metadata":{}},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtypes\n\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:07:50.700623Z","iopub.execute_input":"2021-12-08T03:07:50.701414Z","iopub.status.idle":"2021-12-08T03:07:50.717222Z","shell.execute_reply.started":"2021-12-08T03:07:50.701361Z","shell.execute_reply":"2021-12-08T03:07:50.716273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering\nThese features are borrowed from https://www.kaggle.com/gulshanmishra/tps-dec-21-tensorflow-nn-feature-engineering\nDo read dataset description from https://www.kaggle.com/c/forest-cover-type-prediction/data","metadata":{}},{"cell_type":"code","source":"def feature_engineer(df):\n#     # Euclidean distance to Hydrology\n    df[\"ecldn_dist_hydrlgy\"] = (df[\"Horizontal_Distance_To_Hydrology\"]**2 + df[\"Vertical_Distance_To_Hydrology\"]**2)**0.5\n    # Manhhattan distance to Hydrology\n    df[\"mnhttn_dist_hydrlgy\"] = np.abs(df[\"Horizontal_Distance_To_Hydrology\"]) + np.abs(df[\"Vertical_Distance_To_Hydrology\"])\n    \n#     df['hydro_elevationecldn'] = df['Elevation'] + df['ecldn_dist_hydrlgy'] # ecldn_dist_hydrlgy\n#     df['hydro_elevationmnhttn'] = df['Elevation'] + df['mnhttn_dist_hydrlgy'] # ecldn_dist_hydrlgy\n    df['hydro_elevation'] = df['Elevation'] - df['Vertical_Distance_To_Hydrology'] # ecldn_dist_hydrlgy\n    df['binned_elevation'] = [math.floor(v/50.0) for v in df['Elevation']]\n    \n    df.loc[df[\"Aspect\"] < 0, \"Aspect\"] += 360\n    df.loc[df[\"Aspect\"] > 359, \"Aspect\"] -= 360\n    \n    # Transform Cyclic Ordinal attributes to X y coordinates. 0 degree will be group with 359 degree etc.\n#     df['AspectX'] = np.sin(2.*np.pi*df.Aspect/360)\n#     df['Aspecty'] = np.cos(2.*np.pi*df.Aspect/360)\n\n    soil_features = [x for x in df.columns if x.startswith(\"Soil_Type\")]\n    df[\"soil_type_count\"] = df[soil_features].sum(axis=1)\n\n    wilderness_features = [x for x in df.columns if x.startswith(\"Wilderness_Area\")]\n    df[\"wilderness_area_count\"] = df[wilderness_features].sum(axis=1)\n    \n    df['soil_Type12_32'] = df['Soil_Type32'] + df['Soil_Type12']\n    df['soil_Type23_22_32_33'] = df['Soil_Type23'] + df['Soil_Type22'] + df['Soil_Type32'] + df['Soil_Type33']\n\n    # code to replace all negative value with 0\n    df['Horizontal_Distance_To_Roadways'][df['Horizontal_Distance_To_Roadways'] < 0] = 0\n    df['horizontal_Distance_To_Roadways_Log'] = [math.log(v+1) for v in df['Horizontal_Distance_To_Roadways']]\n    df['Horizontal_Distance_To_Fire_Points'][df['Horizontal_Distance_To_Fire_Points'] < 0] = 0\n    df['horizontal_Distance_To_Fire_Points_Log'] = [math.log(v+1) for v in df['Horizontal_Distance_To_Fire_Points']]\n\n    features_Hillshade = ['Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']\n#     df.loc[df[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] += 360\n#     df.loc[df[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] += 360\n#     df.loc[df[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] += 360\n#     df['Hillshade_9am_AspectX'] = np.sin(2.*np.pi*df.Hillshade_9am/360)\n#     df['Hillshade_9am_Aspecty'] = np.cos(2.*np.pi*df.Hillshade_9am/360)\n#     df['Hillshade_Noon_AspectX'] = np.sin(2.*np.pi*df.Hillshade_Noon/360)\n#     df['Hillshade_Noon_Aspecty'] = np.cos(2.*np.pi*df.Hillshade_Noon/360)\n#     df['Hillshade_3pm_AspectX'] = np.sin(2.*np.pi*df.Hillshade_3pm/360)\n#     df['Hillshade_3pm_Aspecty'] = np.cos(2.*np.pi*df.Hillshade_3pm/360)\n    df.loc[df[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\n    df.loc[df[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\n    df.loc[df[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\n    df.loc[df[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\n    df.loc[df[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\n    df.loc[df[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255\n    df[\"hillshade_mean\"] = df[features_Hillshade].mean(axis=1)\n    df['hillshade_amp'] = df[features_Hillshade].max(axis=1) - df[features_Hillshade].min(axis=1)\n    \n#     df.drop([\"Horizontal_Distance_To_Hydrology\",\"Vertical_Distance_To_Hydrology\"], axis=1, inplace=True)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:07:50.719193Z","iopub.execute_input":"2021-12-08T03:07:50.719703Z","iopub.status.idle":"2021-12-08T03:07:50.735837Z","shell.execute_reply.started":"2021-12-08T03:07:50.719669Z","shell.execute_reply":"2021-12-08T03:07:50.73499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\ndef load_data():\n    # Read data\n    data_dir = Path(INPUT)\n    df_train = pd.read_csv(data_dir / \"train.csv\", index_col=ID)\n    df_test = pd.read_csv(data_dir / \"test.csv\", index_col=ID)\n    # Merge the splits so we can process them together\n#     df = pd.concat([df_train, df_test])\n    # Preprocessing\n#     df = clean(df)\n#     df = encode(df)\n    df_train = impute(df_train)\n    df_test = impute(df_test)\n    df_train = feature_engineer(df_train)\n    df_test = feature_engineer(df_test)\n    df_train = reduce_mem_usage(df_train)\n    df_test = reduce_mem_usage(df_test)\n    \n    # Reform splits\n#     df_train = df.loc[df_train.index, :]\n#     df_test = df.loc[df_test.index, :]\n    return df_train, df_test","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:07:50.737899Z","iopub.execute_input":"2021-12-08T03:07:50.738243Z","iopub.status.idle":"2021-12-08T03:07:50.752736Z","shell.execute_reply.started":"2021-12-08T03:07:50.7382Z","shell.execute_reply":"2021-12-08T03:07:50.751797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data #\n\nAnd now we can call the data loader and get the processed data splits:","metadata":{"papermill":{"duration":0.01628,"end_time":"2021-11-20T07:24:32.26308","exception":false,"start_time":"2021-11-20T07:24:32.2468","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_data, test_data = load_data()","metadata":{"papermill":{"duration":29.900845,"end_time":"2021-11-20T07:24:32.018601","exception":false,"start_time":"2021-11-20T07:24:02.117756","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-08T03:07:50.754147Z","iopub.execute_input":"2021-12-08T03:07:50.75455Z","iopub.status.idle":"2021-12-08T03:08:55.232893Z","shell.execute_reply.started":"2021-12-08T03:07:50.754488Z","shell.execute_reply":"2021-12-08T03:08:55.232011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"column_y = train_data.columns.difference(test_data.columns)[0] # column_y target_col","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:08:55.234095Z","iopub.execute_input":"2021-12-08T03:08:55.234315Z","iopub.status.idle":"2021-12-08T03:08:55.240229Z","shell.execute_reply.started":"2021-12-08T03:08:55.234288Z","shell.execute_reply":"2021-12-08T03:08:55.239266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#customized XY TBR\nidx = train_data[train_data[column_y] == 5].index\ntrain_data.drop(idx, axis=0, inplace=True)\ncols = [\"Soil_Type7\", \"Soil_Type15\"]\ntrain_data.drop(cols, axis=1, inplace=True)\ntest_data.drop(cols, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:08:55.241592Z","iopub.execute_input":"2021-12-08T03:08:55.24231Z","iopub.status.idle":"2021-12-08T03:08:56.407497Z","shell.execute_reply.started":"2021-12-08T03:08:55.242274Z","shell.execute_reply":"2021-12-08T03:08:56.406782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train_data.drop(columns=column_y)\ny = train_data[[column_y]]\n\nX_test = test_data.iloc[:,:]\n\ngc.collect()","metadata":{"papermill":{"duration":0.194437,"end_time":"2021-11-20T07:24:32.230256","exception":false,"start_time":"2021-11-20T07:24:32.035819","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-08T03:08:56.409971Z","iopub.execute_input":"2021-12-08T03:08:56.410174Z","iopub.status.idle":"2021-12-08T03:08:56.753744Z","shell.execute_reply.started":"2021-12-08T03:08:56.410149Z","shell.execute_reply":"2021-12-08T03:08:56.752932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Undersampling\nFor experiment measurements","metadata":{}},{"cell_type":"markdown","source":"## For quick test\nTomekLinks\n(array([1, 2, 3, 4, 5, 6, 7]),\n array([1390684, 2169226,  155218,     218,       1,    7627,   38757]))","metadata":{}},{"cell_type":"code","source":"# Check NA\nmissing_val = X.isnull().sum()\nprint(missing_val[missing_val > 0])\n\nfrom sklearn.model_selection import train_test_split \n# For small testing batch\n# X, X_val, y, y_val = train_test_split(X, y, train_size = VALIDATION_SPLIT, random_state = RANDOM_STATE)\n# X = X.sample(n=SAMPLE, random_state=RANDOM_STATE)\n# y = y.sample(n=SAMPLE, random_state=RANDOM_STATE)\n# X_test = X_test.sample(n=SAMPLE, random_state=RANDOM_STATE)\n\n# undersample = TomekLinks(sampling_strategy='auto') #, random_state=RANDOM_STATE not minority RandomUnderSampler\n# X_under, y_under = undersample.fit_resample(X, y)","metadata":{"papermill":{"duration":1.022769,"end_time":"2021-11-20T07:24:33.302503","exception":false,"start_time":"2021-11-20T07:24:32.279734","status":"completed"},"tags":[],"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-12-08T03:08:56.754828Z","iopub.execute_input":"2021-12-08T03:08:56.755066Z","iopub.status.idle":"2021-12-08T03:08:57.077491Z","shell.execute_reply.started":"2021-12-08T03:08:56.755039Z","shell.execute_reply":"2021-12-08T03:08:57.076611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Uncomment to peek at samples across all targets.\nsmall_sampling = train_data.groupby(column_y).apply(lambda s: s.sample(min(len(s), 977)))\nsmall_sampling","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:08:57.078738Z","iopub.execute_input":"2021-12-08T03:08:57.078977Z","iopub.status.idle":"2021-12-08T03:08:57.828687Z","shell.execute_reply.started":"2021-12-08T03:08:57.078949Z","shell.execute_reply":"2021-12-08T03:08:57.828055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_features(features):\n    for feature in features:\n        plt.rcParams[\"figure.figsize\"] = (15,5)\n        for ctype in list(small_sampling[column_y].unique()):\n            values = small_sampling.loc[small_sampling[column_y] == ctype][feature].values\n            plt.scatter(x=values, y=np.arange(values.size), label=f\"{column_y} {ctype}\")\n        plt.title(feature)\n        plt.legend()\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:25:32.620238Z","iopub.execute_input":"2021-12-08T03:25:32.620805Z","iopub.status.idle":"2021-12-08T03:25:32.628356Z","shell.execute_reply.started":"2021-12-08T03:25:32.620767Z","shell.execute_reply":"2021-12-08T03:25:32.627333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scatter plots for features with contnous values , 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'horizontal_Distance_To_Fire_Points_Log', 'hillshade_mean', 'hillshade_amp']\nfeatures_cols = ['Elevation',\n                 'hydro_elevation',\n                 'binned_elevation']\n\nplot_features(features_cols)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:25:34.534156Z","iopub.execute_input":"2021-12-08T03:25:34.534435Z","iopub.status.idle":"2021-12-08T03:25:36.739615Z","shell.execute_reply.started":"2021-12-08T03:25:34.534405Z","shell.execute_reply":"2021-12-08T03:25:36.738625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampling_key, sampling_count = np.unique(y, return_counts=True)\nsampling_count[sampling_count > SAMPLE] = SAMPLE\nzip_iterator = zip(sampling_key, sampling_count)\nsampling_params = dict(zip_iterator)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:09:00.026306Z","iopub.execute_input":"2021-12-08T03:09:00.026521Z","iopub.status.idle":"2021-12-08T03:09:00.128271Z","shell.execute_reply.started":"2021-12-08T03:09:00.026494Z","shell.execute_reply":"2021-12-08T03:09:00.127286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# not minority\nundersample = RandomUnderSampler(\n    sampling_strategy=sampling_params, random_state=RANDOM_STATE)\n\nX, y = undersample.fit_resample(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:09:00.129642Z","iopub.execute_input":"2021-12-08T03:09:00.129976Z","iopub.status.idle":"2021-12-08T03:09:02.438433Z","shell.execute_reply.started":"2021-12-08T03:09:00.129935Z","shell.execute_reply":"2021-12-08T03:09:02.437557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.unique(y, return_counts=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:09:02.43981Z","iopub.execute_input":"2021-12-08T03:09:02.440228Z","iopub.status.idle":"2021-12-08T03:09:02.449208Z","shell.execute_reply.started":"2021-12-08T03:09:02.440182Z","shell.execute_reply":"2021-12-08T03:09:02.448385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"y to categorical","metadata":{}},{"cell_type":"code","source":"# Prepare for multiclass classification tf.keras.utils.to_categorical(le.fit_transform(y[column_y])) categorical_crossentropy\ny_cat = le.fit_transform(y[column_y])","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:09:02.450553Z","iopub.execute_input":"2021-12-08T03:09:02.450777Z","iopub.status.idle":"2021-12-08T03:09:02.458661Z","shell.execute_reply.started":"2021-12-08T03:09:02.45075Z","shell.execute_reply":"2021-12-08T03:09:02.457943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_num = X.select_dtypes(include=[np.number])\n# X_cat = X.select_dtypes(include=['object'])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-08T03:09:02.459655Z","iopub.execute_input":"2021-12-08T03:09:02.4603Z","iopub.status.idle":"2021-12-08T03:09:02.466806Z","shell.execute_reply.started":"2021-12-08T03:09:02.460261Z","shell.execute_reply":"2021-12-08T03:09:02.465959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.preprocessing import LabelEncoder\n# le = LabelEncoder()\n\n# for i in range(1,5):\n#     X[f'Wilderness_Area{i}'] = le.fit_transform(Xf'Wilderness_Area{i}'])\n#     X_test[f'Wilderness_Area{i}'] = le.fit_transform(X_test[f'Wilderness_Area{i}'])\n# for i in range(1,41):\n#     X[f'Soil_Type{i}'] = le.fit_transform(X[f'Soil_Type{i}'])\n#     X_test[f'Soil_Type{i}'] = le.fit_transform(X_test[f'Soil_Type{i}'])","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-12-08T03:09:02.467849Z","iopub.execute_input":"2021-12-08T03:09:02.468363Z","iopub.status.idle":"2021-12-08T03:09:02.477667Z","shell.execute_reply.started":"2021-12-08T03:09:02.468314Z","shell.execute_reply":"2021-12-08T03:09:02.47708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scaler transformer\nBy using RobustScaler(), we can remove the outliers\n![](https://github.com/furyhawk/kaggle_practice/blob/main/images/Scalers.png?raw=true)","metadata":{"papermill":{"duration":0.016574,"end_time":"2021-11-20T07:24:33.336153","exception":false,"start_time":"2021-11-20T07:24:33.319579","status":"completed"},"tags":[]}},{"cell_type":"code","source":"transformer_all_cols = make_pipeline(\n    RobustScaler(),\n#     StandardScaler(),\n#     MinMaxScaler(feature_range=(0, 1))\n)\n\npreprocessor = make_column_transformer(\n    (transformer_all_cols, X.columns[:]),\n)","metadata":{"papermill":{"duration":0.033189,"end_time":"2021-11-20T07:24:33.38619","exception":false,"start_time":"2021-11-20T07:24:33.353001","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-08T03:09:02.479002Z","iopub.execute_input":"2021-12-08T03:09:02.479402Z","iopub.status.idle":"2021-12-08T03:09:02.488425Z","shell.execute_reply.started":"2021-12-08T03:09:02.479359Z","shell.execute_reply":"2021-12-08T03:09:02.487632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Model and Create Submissions #\n\nOnce you're satisfied with everything, it's time to create your final predictions! This cell will:\n- use the best trained model to make predictions from the test set\n- save the predictions to a CSV file\n\n$Softmax: \\sigma(z_i) = \\frac{e^{z_{i}}}{\\sum_{j=1}^K e^{z_{j}}} \\ \\ \\ for\\ i=1,2,\\dots,K$\n","metadata":{"papermill":{"duration":0.017548,"end_time":"2021-11-20T07:24:33.421367","exception":false,"start_time":"2021-11-20T07:24:33.403819","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# try: # detect TPUs\n#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n#     strategy = tf.distribute.TPUStrategy(tpu)\n# except ValueError: # detect GPUs\n# #     strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n#     #strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n#     strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() # for clusters of multi-GPU machines\n\n# print(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n# BATCH_SIZE = 16 * strategy.num_replicas_in_sync # this is 8 on TPU v3-8, it is 1 on CPU and GPU\n\ndef load_model():\n    \n    early_stopping = EarlyStopping(\n        patience=ES_PATIENCE,\n        min_delta=0,\n        monitor= ACC_VAL_METRICS,\n        mode='max',\n        restore_best_weights=True,       \n        baseline=None,\n        verbose=VERBOSE,\n    )\n    plateau = ReduceLROnPlateau(\n            patience=RLRP_PATIENCE,\n            factor=LR_FACTOR,\n            monitor='val_loss', \n            mode='min',\n            verbose=VERBOSE,\n    )\n\n# -----------------------------------------------------------------\n# Model , kernel_initializer=\"lecun_normal\"\n    # instantiating the model in the strategy scope creates the model on the TPU\n#     with strategy.scope():\n    model = keras.Sequential([\n    layers.BatchNormalization(input_shape = [X.shape[-1]], name='input'),\n    layers.Dense(300, kernel_initializer=\"lecun_normal\", activation=ACTIVATION),\n    layers.Dropout(rate = DROPOUT),\n    layers.BatchNormalization(),\n    layers.Dense(200, kernel_initializer=\"lecun_normal\", activation=ACTIVATION),\n    layers.Dropout(rate = DROPOUT),\n    layers.BatchNormalization(),\n    layers.Dense(100, kernel_initializer=\"lecun_normal\", activation=ACTIVATION),\n    layers.Dropout(rate = DROPOUT),\n    layers.BatchNormalization(),\n    layers.Dense(50, kernel_initializer=\"lecun_normal\", activation=ACTIVATION),\n    layers.Dropout(rate = DROPOUT),\n    layers.BatchNormalization(),\n    layers.Dense(le.classes_.shape[-1], activation = 'softmax'), #y_cat.shape[-1]\n    ])\n\n# -----------------------------------------------------------------\n# sparse_categorical_crossentropy\n    model.compile(\n        optimizer=OPTIMIZER, #tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE), Adamax adam\n        loss=LOSS, # sparse_categorical_crossentropy categorical_crossentropy\n        metrics=[METRICS], # acc sparse_categorical_accuracy\n    )\n\n    return model, early_stopping, plateau","metadata":{"papermill":{"duration":5.927224,"end_time":"2021-11-20T07:24:39.365437","exception":false,"start_time":"2021-11-20T07:24:33.438213","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-08T03:09:02.489996Z","iopub.execute_input":"2021-12-08T03:09:02.4902Z","iopub.status.idle":"2021-12-08T03:09:02.504599Z","shell.execute_reply.started":"2021-12-08T03:09:02.490176Z","shell.execute_reply":"2021-12-08T03:09:02.503845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Performance Measures\n## StratifiedKFold\nPerform stratified sampling to produce folds that contain a representative ratio of each class.","metadata":{"papermill":{"duration":0.016357,"end_time":"2021-11-20T07:24:39.398757","exception":false,"start_time":"2021-11-20T07:24:39.3824","status":"completed"},"tags":[]}},{"cell_type":"code","source":"kf = StratifiedKFold(n_splits=FOLDS,random_state=RANDOM_STATE,shuffle=True)","metadata":{"papermill":{"duration":0.024579,"end_time":"2021-11-20T07:24:39.44106","exception":false,"start_time":"2021-11-20T07:24:39.416481","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-08T03:09:02.506041Z","iopub.execute_input":"2021-12-08T03:09:02.506295Z","iopub.status.idle":"2021-12-08T03:09:02.518659Z","shell.execute_reply.started":"2021-12-08T03:09:02.506267Z","shell.execute_reply":"2021-12-08T03:09:02.517929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_transformed = preprocessor.fit_transform(X)\nX_test = preprocessor.transform(test_data)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:09:02.519964Z","iopub.execute_input":"2021-12-08T03:09:02.520195Z","iopub.status.idle":"2021-12-08T03:09:03.008022Z","shell.execute_reply.started":"2021-12-08T03:09:02.520167Z","shell.execute_reply":"2021-12-08T03:09:03.00711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# resets\npreds_valid_f = {}\npreds_test = np.zeros((1, 1)) # []\ntotal_acc = []\nf_scores = []\n\nfor fold,(train_index, valid_index) in enumerate(kf.split(X=X, y=y.Cover_Type)):\n\n    X_train, X_valid = X_train_transformed[train_index], X_train_transformed[valid_index]\n    y_train, y_valid = y_cat[train_index], y_cat[valid_index]\n\n    #   --------------------------------------------------------  \n    # Preprocessing\n    index_valid = valid_index.tolist()\n#     index_valid  = X_valid.index.tolist()\n#     X_train = preprocessor.fit_transform(X_train)\n#     X_valid = preprocessor.transform(X_valid)\n    \n    #  ----------------------------------------------------------    \n    # Model\n    model, early_stopping, plateau  = load_model()\n\n    history = model.fit( X_train, y_train,\n                validation_data = (X_valid, y_valid),\n                batch_size      = BATCH_SIZE, \n                epochs          = EPOCHS,\n                callbacks       = [early_stopping, plateau],\n                shuffle         = True,\n                verbose         = VERBOSE,\n              )\n    \n    #  ----------------------------------------------------------\n    #  oof\n    preds_valid = model.predict(X_valid, batch_size=BATCH_SIZE)\n    \n    #  ----------------------------------------------------------\n    #  test dataset predictions for submission np.argmax(, axis=1) + 1\n#     preds_test.append( le.inverse_transform(np.argmax(model.predict(X_test, batch_size=BATCH_SIZE), axis=1)) )\n    preds_test = preds_test + model.predict(X_test, batch_size=BATCH_SIZE)\n\n    #  ----------------------------------------------------------\n    #  Saving  scores to plot the end  \n    scores = pd.DataFrame(history.history)\n    scores['folds'] = fold\n    if fold == 0:\n        f_scores = scores\n        model.summary()\n    else: \n        f_scores = pd.concat([f_scores, scores], axis  = 0)\n\n    #  ----------------------------------------------------------\n    #  concatenating valid preds\n    preds_valid_f.update(dict(zip(index_valid, le.inverse_transform(np.argmax(preds_valid, axis=1)))))\n    # Getting score for a fold model\n    fold_acc = accuracy_score(y.iloc[valid_index].Cover_Type, le.inverse_transform(np.argmax(preds_valid, axis=1)))\n    print(f\"Fold {fold} accuracy_score: {fold_acc}\")\n\n    # Total acc\n    total_acc.append(fold_acc)\n    \n    del model\n    gc.collect()\n\nprint(f\"mean accuracy_score: {np.mean(total_acc)}, std: {np.std(total_acc)}\")","metadata":{"papermill":{"duration":2192.762948,"end_time":"2021-11-20T08:01:12.221168","exception":false,"start_time":"2021-11-20T07:24:39.45822","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-08T03:09:03.009579Z","iopub.execute_input":"2021-12-08T03:09:03.009841Z","iopub.status.idle":"2021-12-08T03:19:06.642365Z","shell.execute_reply.started":"2021-12-08T03:09:03.009808Z","shell.execute_reply":"2021-12-08T03:19:06.641476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{"papermill":{"duration":0.018523,"end_time":"2021-11-20T08:01:12.258711","exception":false,"start_time":"2021-11-20T08:01:12.240188","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def plot_acc(f_scores):\n    for fold in range(f_scores['folds'].nunique()):\n        history_f = f_scores[f_scores['folds'] == fold]\n        \n        best_epoch = np.argmin(np.array(history_f['val_loss']))\n        best_val_loss = history_f['val_loss'][best_epoch]\n\n        fig, ax1 = plt.subplots(1, 2, tight_layout=True, figsize=(15,4))\n\n        fig.suptitle('Fold : '+ str(fold+1) +\n                     \" Validation Loss: {:0.4f}\".format(history_f['val_loss'].min()) +\n                     \" Validation Accuracy: {:0.4f}\".format(history_f[ACC_VAL_METRICS].max()) +\n                     \" LR: {:0.8f}\".format(history_f['lr'].min())\n                     , fontsize=14)\n\n        plt.subplot(1,2,1)\n        plt.plot(history_f.loc[:, ['loss', 'val_loss']], label= ['loss', 'val_loss'])\n                \n        from_epoch = 0\n        if best_epoch >= from_epoch:\n            plt.scatter([best_epoch], [best_val_loss], c='r', label=f'Best val_loss = {best_val_loss:.5f}')\n        if best_epoch > 0:\n            almost_epoch = np.argmin(np.array(history_f['val_loss'])[:best_epoch])\n            almost_val_loss = history_f['val_loss'][almost_epoch]\n            if almost_epoch >= from_epoch:\n                plt.scatter([almost_epoch], [almost_val_loss], c='orange', label='Second best val_loss')\n        plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.legend(loc='upper left')        \n        \n        ax2 = plt.gca().twinx()\n        ax2.plot(history_f.loc[:, ['lr']], 'y:', label='lr' ) # default color is same as first ax\n        ax2.set_ylabel('Learning rate')\n        ax2.legend(loc='upper right')\n        ax2.grid()\n\n        best_epoch = np.argmax(np.array(history_f[ACC_VAL_METRICS]))\n        best_val_acc = history_f[ACC_VAL_METRICS][best_epoch]\n        \n        plt.subplot(1,2,2)\n        plt.plot(history_f.loc[:, [ACC_METRICS, ACC_VAL_METRICS]],label= [ACC_METRICS, ACC_VAL_METRICS])\n        if best_epoch >= from_epoch:\n            plt.scatter([best_epoch], [best_val_acc], c='r', label=f'Best val_acc = {best_val_acc:.5f}')\n        plt.xlabel('Epoch')\n        plt.ylabel('Accuracy')\n        plt.legend(loc='lower left')    \n        plt.legend(fontsize=15)\n        plt.grid(b=True, linestyle='-')\n\nplot_acc(f_scores)","metadata":{"papermill":{"duration":3.011176,"end_time":"2021-11-20T08:01:15.289717","exception":false,"start_time":"2021-11-20T08:01:12.278541","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-08T03:19:06.645472Z","iopub.execute_input":"2021-12-08T03:19:06.646047Z","iopub.status.idle":"2021-12-08T03:19:10.915949Z","shell.execute_reply.started":"2021-12-08T03:19:06.645998Z","shell.execute_reply":"2021-12-08T03:19:10.914847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(3, 5))\nplt.plot([1]*FOLDS, total_acc, \".\")\nplt.boxplot([total_acc], labels=(\"y\"))\nplt.ylabel(\"Accuracy\", fontsize=14)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-08T03:19:10.917284Z","iopub.execute_input":"2021-12-08T03:19:10.917523Z","iopub.status.idle":"2021-12-08T03:19:11.149091Z","shell.execute_reply.started":"2021-12-08T03:19:10.917494Z","shell.execute_reply":"2021-12-08T03:19:11.148146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Confusion matrix\n\n$Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}$\n\n$Precision = \\frac{TP}{TP+FP}$\n\n$Recall = \\frac{TP}{TP+FN}$\n\n$F1 = \\frac{2*Precision*Recall}{Precision+Recall} = \\frac{2*TP}{2*TP+FP+FN}$\n\n","metadata":{}},{"cell_type":"code","source":"def plot_cm(cm):\n    metrics = {\n        'accuracy': cm / cm.sum(),\n        'recall' : cm / cm.sum(axis=1),\n        'precision': cm / cm.sum(axis=0)\n    }\n    \n    fig, ax = plt.subplots(1,3, tight_layout=True, figsize=(15,5))\n    ax = ax.flatten()\n\n    mask = (np.eye(cm.shape[0]) == 0) * 1\n\n    for idx, (name, matrix) in enumerate(metrics.items()):\n\n        ax[idx].set_title(name)\n\n        sns.heatmap(\n            data=matrix,\n            cmap=sns.dark_palette(\"#69d\", reverse=True, as_cmap=True),\n            cbar=False,\n#             mask=mask,\n            lw=0.25,\n            annot=True,\n            fmt='.2f',\n            ax=ax[idx]\n        )\n    sns.despine()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-08T03:19:11.152587Z","iopub.execute_input":"2021-12-08T03:19:11.153291Z","iopub.status.idle":"2021-12-08T03:19:11.162433Z","shell.execute_reply.started":"2021-12-08T03:19:11.15325Z","shell.execute_reply":"2021-12-08T03:19:11.161772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_y_hat = []\nfor key, value in sorted(preds_valid_f.items()):\n    oof_y_hat.append(value)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-08T03:19:11.163963Z","iopub.execute_input":"2021-12-08T03:19:11.164453Z","iopub.status.idle":"2021-12-08T03:19:11.210781Z","shell.execute_reply.started":"2021-12-08T03:19:11.164409Z","shell.execute_reply":"2021-12-08T03:19:11.209648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create confusion matrix, calculate accuracy,recall & precision\ncm = pd.DataFrame(data=confusion_matrix(y, oof_y_hat, labels=le.classes_), index=le.classes_, columns=le.classes_)\nplot_cm(cm)","metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-08T03:19:11.212069Z","iopub.execute_input":"2021-12-08T03:19:11.21233Z","iopub.status.idle":"2021-12-08T03:19:12.608026Z","shell.execute_reply.started":"2021-12-08T03:19:11.212298Z","shell.execute_reply":"2021-12-08T03:19:12.606936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(y, oof_y_hat, labels=le.classes_)","metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-08T03:19:12.609375Z","iopub.execute_input":"2021-12-08T03:19:12.609621Z","iopub.status.idle":"2021-12-08T03:19:12.719997Z","shell.execute_reply.started":"2021-12-08T03:19:12.60959Z","shell.execute_reply":"2021-12-08T03:19:12.719351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"But instead of just looking at the mean accuracy across the 10 cross-validation folds, let's plot all 10 scores for each model, along with a box plot highlighting the lower and upper quartiles, and \"whiskers\" showing the extent of the scores. Note that the `boxplot()` function detects outliers (called \"fliers\") and does not include them within the whiskers. Specifically, if the lower quartile is $Q_1$ and the upper quartile is $Q_3$, then the interquartile range $IQR = Q_3 - Q_1$ (this is the box's height), and any score lower than $Q_1 - 1.5 \\times IQR$ is a flier, and so is any score greater than $Q3 + 1.5 \\times IQR$.","metadata":{}},{"cell_type":"code","source":"preds_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:19:12.720928Z","iopub.execute_input":"2021-12-08T03:19:12.721467Z","iopub.status.idle":"2021-12-08T03:19:12.726348Z","shell.execute_reply.started":"2021-12-08T03:19:12.721435Z","shell.execute_reply":"2021-12-08T03:19:12.725458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# le.inverse_transform(np.argmax(preds_test, axis=1))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-08T03:19:12.727455Z","iopub.execute_input":"2021-12-08T03:19:12.727686Z","iopub.status.idle":"2021-12-08T03:19:12.739852Z","shell.execute_reply.started":"2021-12-08T03:19:12.727658Z","shell.execute_reply":"2021-12-08T03:19:12.739167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{"papermill":{"duration":0.025848,"end_time":"2021-11-20T08:01:15.342765","exception":false,"start_time":"2021-11-20T08:01:15.316917","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# preds_test = np.argmax(preds_test, axis=1)\n# preds_test = le.inverse_transform(preds_test)\n\nsub = pd.read_csv(INPUT + '/sample_submission.csv')\nsub[column_y] = le.inverse_transform(np.argmax(preds_test, axis=1)) # (stats.mode(preds_test)[0][0]) preds_test[FOLDS-1] # argmax reverse of to_categorical sub[column_y] = (np.argmax(sum(preds_test), axis=1) + 1)\nsub.to_csv('submission.csv', index=False)\nsub","metadata":{"papermill":{"duration":1.761381,"end_time":"2021-11-20T08:01:17.130128","exception":false,"start_time":"2021-11-20T08:01:15.368747","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-08T03:19:12.741158Z","iopub.execute_input":"2021-12-08T03:19:12.742211Z","iopub.status.idle":"2021-12-08T03:19:14.968669Z","shell.execute_reply.started":"2021-12-08T03:19:12.742158Z","shell.execute_reply":"2021-12-08T03:19:14.967784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.unique(sub[column_y], return_counts=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:19:14.971827Z","iopub.execute_input":"2021-12-08T03:19:14.972109Z","iopub.status.idle":"2021-12-08T03:19:15.003638Z","shell.execute_reply.started":"2021-12-08T03:19:14.972079Z","shell.execute_reply":"2021-12-08T03:19:15.002947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the distribution of the test predictions vs training set\nplt.figure(figsize=(10,5))\nplt.hist(train_data[column_y], bins=np.linspace(0.5, 7.5, 8), density=True, label='Training labels')\nplt.hist(sub[column_y], bins=np.linspace(0.5, 7.5, 8), density=True, rwidth=0.7, label='Test predictions')\nplt.xlabel(column_y)\nplt.ylabel('Frequency')\nplt.gca().yaxis.set_major_formatter(PercentFormatter())\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:19:15.006687Z","iopub.execute_input":"2021-12-08T03:19:15.007687Z","iopub.status.idle":"2021-12-08T03:19:15.43743Z","shell.execute_reply.started":"2021-12-08T03:19:15.007635Z","shell.execute_reply":"2021-12-08T03:19:15.436844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the distribution of the test predictions\nfig, ax =plt.subplots(1,2,figsize=(10,4))\nsns.countplot(x = sub[column_y], ax=ax[0], orient=\"h\").set_title(\"Prediction\")\n# Plot the distribution of the training set\nsns.countplot(x = train_data[column_y], ax=ax[1], orient=\"h\").set_title(\"Training labels\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:19:15.438582Z","iopub.execute_input":"2021-12-08T03:19:15.438963Z","iopub.status.idle":"2021-12-08T03:19:16.15205Z","shell.execute_reply.started":"2021-12-08T03:19:15.43893Z","shell.execute_reply":"2021-12-08T03:19:16.150945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"keys_list, values_list = np.unique(my_list, return_counts=True)keys_list, values_list = np.unique(my_list, return_counts=True)To submit these predictions to the competition, follow these steps:\n\n1. Begin by clicking on the blue **Save Version** button in the top right corner of the window.  This will generate a pop-up window.\n2. Ensure that the **Save and Run All** option is selected, and then click on the blue **Save** button.\n3. This generates a window in the bottom left corner of the notebook.  After it has finished running, click on the number to the right of the **Save Version** button.  This pulls up a list of versions on the right of the screen.  Click on the ellipsis **(...)** to the right of the most recent version, and select **Open in Viewer**.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n4. Click on the **Output** tab on the right of the screen.  Then, click on the file you would like to submit, and click on the blue **Submit** button to submit your results to the leaderboard.\n\nYou have now successfully submitted to the competition!\n\n# Next Steps #\n\nIf you want to keep working to improve your performance, select the blue **Edit** button in the top right of the screen. Then you can change your code and repeat the process. There's a lot of room to improve, and you will climb up the leaderboard as you work.\n\nBe sure to check out [other users' notebooks](https://www.kaggle.com/c/tabular-playground-series-dec-2021/code) in this competition. You'll find lots of great ideas for new features and as well as other ways to discover more things about the dataset or make better predictions. There's also the [discussion forum](https://www.kaggle.com/c/tabular-playground-series-dec-2021/discussion), where you can share ideas with other Kagglers.\n\nHave fun Kaggling!","metadata":{}},{"cell_type":"code","source":"# np.array(preds_test).shape","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-08T03:19:16.153527Z","iopub.execute_input":"2021-12-08T03:19:16.154206Z","iopub.status.idle":"2021-12-08T03:19:16.157496Z","shell.execute_reply.started":"2021-12-08T03:19:16.154163Z","shell.execute_reply":"2021-12-08T03:19:16.156808Z"},"trusted":true},"execution_count":null,"outputs":[]}]}