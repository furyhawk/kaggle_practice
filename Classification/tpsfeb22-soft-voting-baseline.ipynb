{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Welcome and have fun learning multiclass classification\n\n#### Metric: **Accuracy**. Softvoting and weighted average is the objective to score towards target class.\n\nObjective of this notebook used to be a ~simple~ and robust neural network multiclass classifier for future use.\n\nTODO: XGB even in KFOLD will overfit.\n\n<blockquote style=\"margin-right:auto; margin-left:auto; padding: 1em; margin:24px;\">\n    <strong>Fork This Notebook!</strong><br>\nCreate your own editable copy of this notebook by clicking on the <strong>Copy and Edit</strong> button in the top right corner.\n</blockquote>","metadata":{}},{"cell_type":"code","source":"if '__initialized__' not in locals():\n    !pip install scikit-learn -U\n    # Intel® Extension for Scikit-learn installation:\n    !pip install scikit-learn-intelex\n    from sklearnex import patch_sklearn\n    patch_sklearn()\n\n# Installation Initialized\n__initialized__ = True","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-08T02:36:40.897323Z","iopub.execute_input":"2022-02-08T02:36:40.897681Z","iopub.status.idle":"2022-02-08T02:36:40.905343Z","shell.execute_reply.started":"2022-02-08T02:36:40.897644Z","shell.execute_reply":"2022-02-08T02:36:40.904631Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\nfrom scipy import stats\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport dask.dataframe as dd\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\nimport seaborn as sns\n\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler, TomekLinks\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, RobustScaler, PowerTransformer, OneHotEncoder\nle = LabelEncoder()\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.metrics import accuracy_score, log_loss, confusion_matrix, roc_curve, precision_recall_curve\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split\nfrom sklearn.decomposition import PCA\n\nfrom datetime import datetime\nfrom packaging import version\n\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport gc\nimport os\nimport math\nimport random\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T02:36:40.907096Z","iopub.execute_input":"2022-02-08T02:36:40.907511Z","iopub.status.idle":"2022-02-08T02:36:40.924895Z","shell.execute_reply.started":"2022-02-08T02:36:40.907465Z","shell.execute_reply":"2022-02-08T02:36:40.924153Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"markdown","source":"## Fine tuning\nFine tune the system using the hyperparameters and configs below:\n* **PRODUCTION** - True: For submission run. False: Fast trial run\n* FOLD - 5, 10, 15, 20.\n* SAMPLE - Set it to True for full sample run. Max sample per class.\n* N_ESTIMATORS - Model hyperparameter","metadata":{}},{"cell_type":"code","source":"# -----------------------------------------------------------------\n# Some parameters to config \nPRODUCTION = False # True: For submission run. False: Fast trial run\n\n# Hyperparameters\nFOLDS = 10 if PRODUCTION else 5   # Only 5 or 10.\nN_ESTIMATORS = 2100 if PRODUCTION else 300 # Overfitting vs underfitting https://www.kaggle.com/c/tabular-playground-series-feb-2022/discussion/305463\nAVERAGE_WEIGHTED_FOLD = False\n\n# The dataset is too huge for trial. Sampling it for speed run!\nSAMPLE = 20139 if PRODUCTION else 12522   # Max Sample size per category. For quick test: y counts [12522, 20139, 20063, 19947, 19958, 19937, 19847, 20030, 19929, 20074, 20076]  # 200000 total rows\nVALIDATION_SPLIT = 0.25 # Only used to min dataset for quick test\n\nRANDOM_STATE = 321\nVERBOSE = 0\n\n# Admin\nID = \"row_id\"            # Id id x X index\nINPUT = \"../input/tabular-playground-series-feb-2022\"\nTPU = False           # True: use TPU.\nGPU = False           # True: use GPU.\nBEST_OR_FOLD = False # True: use Best model, False: use KFOLD softvote\nFEATURE_ENGINEERING = True\nPSEUDO_LABEL = False\nBLEND = False\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T02:36:40.926168Z","iopub.execute_input":"2022-02-08T02:36:40.926392Z","iopub.status.idle":"2022-02-08T02:36:40.944956Z","shell.execute_reply.started":"2022-02-08T02:36:40.926365Z","shell.execute_reply":"2022-02-08T02:36:40.944211Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\nseed_everything(RANDOM_STATE)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T02:36:40.946583Z","iopub.execute_input":"2022-02-08T02:36:40.947002Z","iopub.status.idle":"2022-02-08T02:36:40.962524Z","shell.execute_reply.started":"2022-02-08T02:36:40.946955Z","shell.execute_reply":"2022-02-08T02:36:40.961924Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"def plot_cm(cm):\n    metrics = {\n        'accuracy': cm / cm.sum(),\n        'recall' : cm / cm.sum(axis =1 ),\n        'precision': cm / cm.sum(axis = 0)\n    }\n    \n    fig, ax = plt.subplots(1,3, tight_layout = True, figsize = (20,6))\n    ax = ax.flatten()\n#     mask = (np.eye(cm.shape[0]) == 0) * 1\n    for idx, (name, matrix) in enumerate(metrics.items()):\n        ax[idx].set_title(name)\n        sns.heatmap(\n            data = matrix,\n            cmap = sns.dark_palette(\"#69d\", reverse=True, as_cmap=True),\n            cbar = False,\n#             mask=mask,\n            lw = 0.25,\n            annot = True,\n            fmt = '.2f',\n            ax = ax[idx]\n        )\n#         for tick in ax[idx].get_xticklabels():\n#                 tick.set_rotation(60)\n                \n    sns.despine()\n    \ndef plot_cm_error(cm):\n    mask = (np.eye(cm.shape[0]) != 0) * 1\n    fig, ax = plt.subplots(tight_layout=True, figsize=(15,8))\n    sns.heatmap(\n                data = pd.DataFrame(data=cm, index=le.classes_, columns = le.classes_),\n#                 cmap=sns.dark_palette(\"#69d\", reverse=True, as_cmap=True),\n                cbar = False,\n                lw = 0.25,\n                mask = mask,\n                annot = True,\n                fmt = '.0f',\n                ax = ax\n            )\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    sns.despine()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T02:36:40.963695Z","iopub.execute_input":"2022-02-08T02:36:40.964068Z","iopub.status.idle":"2022-02-08T02:36:40.977558Z","shell.execute_reply.started":"2022-02-08T02:36:40.964031Z","shell.execute_reply":"2022-02-08T02:36:40.976794Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"markdown","source":"## Reduce Memory usage","metadata":{}},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtypes\n\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n \n    return df","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T02:36:41.019848Z","iopub.execute_input":"2022-02-08T02:36:41.020418Z","iopub.status.idle":"2022-02-08T02:36:41.035356Z","shell.execute_reply.started":"2022-02-08T02:36:41.020367Z","shell.execute_reply":"2022-02-08T02:36:41.034256Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\n\n\ndef load_data():\n    # Read data\n    data_dir = Path(INPUT)\n    try:\n        # Read the parquet data.\n        df_train = pd.read_parquet('train.parquet').pipe(reduce_mem_usage)\n        df_test = pd.read_parquet('test.parquet').pipe(reduce_mem_usage)\n    except FileNotFoundError:\n        df_train = pd.read_csv(data_dir / \"train.csv\", index_col=ID).pipe(reduce_mem_usage)\n        df_test = pd.read_csv(data_dir / \"test.csv\", index_col=ID).pipe(reduce_mem_usage)\n#         df_train.drop_duplicates(keep='first', inplace=True)\n    # Save the csv file to parquet.\n    # I learned parquet from this notebook: https://www.kaggle.com/wti200/one-vs-rest-approach\n    df_train.to_parquet('train.parquet')\n    df_test.to_parquet('test.parquet')\n    column_y = df_train.columns.difference(\n        df_test.columns)[0]  # column_y target_col label_col\n    return df_train, df_test, column_y","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T02:36:41.037598Z","iopub.execute_input":"2022-02-08T02:36:41.038338Z","iopub.status.idle":"2022-02-08T02:36:41.052751Z","shell.execute_reply.started":"2022-02-08T02:36:41.038294Z","shell.execute_reply":"2022-02-08T02:36:41.052122Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing ##\n\nBefore we can do any feature engineering, we need to *preprocess* the data to get it in a form suitable for analysis. We'll need to:\n- **Load** the data from CSV files\n- **Clean** the data to fix any errors or inconsistencies\n- **Encode** the statistical data type (numeric, categorical)\n- **Impute** any missing values\n\nWe'll wrap all these steps up in a function, which will make easy for you to get a fresh dataframe whenever you need. After reading the CSV file, we'll apply three preprocessing steps, `clean`, `encode`, and `impute`, and then create the data splits: one (`df_train`) for training the model, and one (`df_test`) for making the predictions that you'll submit to the competition for scoring on the leaderboard.","metadata":{}},{"cell_type":"code","source":"%%time\ntrain_data, test_data, TARGET_FEATURE_NAME = load_data()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T02:36:41.054176Z","iopub.execute_input":"2022-02-08T02:36:41.055025Z","iopub.status.idle":"2022-02-08T02:36:45.918307Z","shell.execute_reply.started":"2022-02-08T02:36:41.054981Z","shell.execute_reply":"2022-02-08T02:36:45.917355Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"# Check NA\nmissing_val = train_data.isnull().sum()\nprint(missing_val[missing_val > 0])","metadata":{"execution":{"iopub.status.busy":"2022-02-08T02:36:45.920334Z","iopub.execute_input":"2022-02-08T02:36:45.920690Z","iopub.status.idle":"2022-02-08T02:36:46.056595Z","shell.execute_reply.started":"2022-02-08T02:36:45.920643Z","shell.execute_reply":"2022-02-08T02:36:46.055458Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"markdown","source":"## Duplicate rows check\nhttps://www.kaggle.com/sfktrkl/tps-feb-2022/notebook","metadata":{}},{"cell_type":"code","source":"# Save original target distribution\ntarget_distribution = train_data[TARGET_FEATURE_NAME].value_counts().sort_index() / len(train_data) * 100\n\nduplicates_train = train_data.duplicated().sum()\nprint('Duplicates in train data: {0}'.format(duplicates_train))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T02:36:46.059034Z","iopub.execute_input":"2022-02-08T02:36:46.059318Z","iopub.status.idle":"2022-02-08T02:36:48.052874Z","shell.execute_reply.started":"2022-02-08T02:36:46.059277Z","shell.execute_reply":"2022-02-08T02:36:48.051679Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"markdown","source":"## Dropping duplicated rows\nDropping duplicated rows actually improve LB scores. Look like kernel is overfitting around duplicated rows.","metadata":{}},{"cell_type":"code","source":"train_df = train_data.groupby(list(train_data.columns.values)).size().reset_index(name='sample_weight').copy()\ntrain_df.drop_duplicates(keep='first', inplace=True)\nduplicates_train = train_df.duplicated().sum()\n\nprint('Train data shape:', train_df.shape)\nprint('Duplicates in train data: {0}'.format(duplicates_train))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T02:36:48.054329Z","iopub.execute_input":"2022-02-08T02:36:48.054631Z","iopub.status.idle":"2022-02-08T02:36:55.981319Z","shell.execute_reply.started":"2022-02-08T02:36:48.054588Z","shell.execute_reply":"2022-02-08T02:36:55.980389Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"X = train_df.drop(columns=[TARGET_FEATURE_NAME, 'sample_weight'])\ny = train_df[[TARGET_FEATURE_NAME]]\nsample_weight = train_df['sample_weight']\n\nX_submission = test_data.loc[:,X.columns]\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T02:39:03.844106Z","iopub.execute_input":"2022-02-08T02:39:03.844778Z","iopub.status.idle":"2022-02-08T02:39:04.127716Z","shell.execute_reply.started":"2022-02-08T02:39:03.844725Z","shell.execute_reply":"2022-02-08T02:39:04.126911Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"markdown","source":"# Undersampling\nFor experiment measurements","metadata":{}},{"cell_type":"code","source":"def sampling_size_params(labels, sampling_max_size = SAMPLE):\n    ''' Return sampling parameters {labels: sample_size}'''\n    sampling_key, sampling_count = np.unique(labels, return_counts=True)\n    sampling_count[sampling_count > sampling_max_size] = sampling_max_size\n    zip_iterator = zip(sampling_key, sampling_count)\n    return dict(zip_iterator)\n\n# not minority\nsampling_params = sampling_size_params(y, SAMPLE)\n# sampling_params['Escherichia_coli'] = sampling_params['Escherichia_coli'] + 1\nundersample = RandomUnderSampler(\n    sampling_strategy=sampling_params, random_state=RANDOM_STATE)\n\nX, y = undersample.fit_resample(X, y)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T02:39:04.129178Z","iopub.execute_input":"2022-02-08T02:39:04.129426Z","iopub.status.idle":"2022-02-08T02:39:05.771315Z","shell.execute_reply.started":"2022-02-08T02:39:04.129398Z","shell.execute_reply":"2022-02-08T02:39:05.770313Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"# Prepare for multiclass classification\ny_cat = le.fit_transform(y[TARGET_FEATURE_NAME]) # y to categorical","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T02:39:05.772784Z","iopub.execute_input":"2022-02-08T02:39:05.773431Z","iopub.status.idle":"2022-02-08T02:39:05.815825Z","shell.execute_reply.started":"2022-02-08T02:39:05.773383Z","shell.execute_reply":"2022-02-08T02:39:05.815155Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"CSV_HEADER = list(train_data.columns[:])\n\nTARGET_FEATURE_LABELS = np.unique(y_cat)\n\nNUMERIC_FEATURE_NAMES = list(X.columns[:])\n\nCATEGORICAL_FEATURES_WITH_VOCABULARY = {}\n\nCATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n\nFEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n\nCOLUMN_DEFAULTS = [\n    [0] if feature_name in NUMERIC_FEATURE_NAMES + [TARGET_FEATURE_NAME] else [\"NA\"]\n    for feature_name in CSV_HEADER\n]\n\nNUM_CLASSES = len(TARGET_FEATURE_LABELS)\n\nINPUT_SHAPE = X.shape[-1]\nOUTPUT_SHAPE = le.classes_.shape[-1]\nprint(f'No. of features: {INPUT_SHAPE}')\nprint(f'Output shape: {OUTPUT_SHAPE}')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T02:39:05.817588Z","iopub.execute_input":"2022-02-08T02:39:05.818168Z","iopub.status.idle":"2022-02-08T02:39:05.829466Z","shell.execute_reply.started":"2022-02-08T02:39:05.818130Z","shell.execute_reply":"2022-02-08T02:39:05.828777Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"del train_data\ndel train_df\ngc.collect()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T02:39:05.830734Z","iopub.execute_input":"2022-02-08T02:39:05.830993Z","iopub.status.idle":"2022-02-08T02:39:05.986502Z","shell.execute_reply.started":"2022-02-08T02:39:05.830962Z","shell.execute_reply":"2022-02-08T02:39:05.985680Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"markdown","source":"# Train Model and Create Submissions #\n\nOnce you're satisfied with everything, it's time to create your final predictions! This cell will:\n- use the best trained model to make predictions from the test set\n- save the predictions to a CSV file\n\n$Softmax: \\sigma(z_i) = \\frac{e^{z_{i}}}{\\sum_{j=1}^K e^{z_{j}}} \\ \\ \\ for\\ i=1,2,\\dots,K$\n\nK - number of classes\n\n$z_i$ - is a vector containing the scores of each class for the instance z.\n\n$\\sigma(z_i)$ - is the estimated probability that the instance z belongs to class K, given the scores of each class for that instance.\n\n$Relu(z) = max(0, z)$\n\nBinary Cross Entropy: $-{(y\\log(p) + (1 - y)\\log(1 - p))}$\n\nFor multiclass classification, we calculate a separate loss for each class label per observation and sum the result.\n\n$-\\sum_{c=1}^My_{o,c}\\log(p_{o,c})$\n\n\n    M - number of classes\n\n    log - the natural log\n\n    y - binary indicator (0 or 1) if class label c is the correct classification for observation o\n\n    p - predicted probability observation o is of class c\n\n","metadata":{}},{"cell_type":"code","source":"from sklearn.compose import make_column_selector, ColumnTransformer, TransformedTargetRegressor\n\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, VotingClassifier, StackingClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T02:39:05.988074Z","iopub.execute_input":"2022-02-08T02:39:05.988401Z","iopub.status.idle":"2022-02-08T02:39:05.998984Z","shell.execute_reply.started":"2022-02-08T02:39:05.988356Z","shell.execute_reply":"2022-02-08T02:39:05.998207Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"markdown","source":"## Create Models","metadata":{}},{"cell_type":"code","source":"def build_estimator_stack(estimator_stack=[], seed=RANDOM_STATE):\n    if GPU:\n        param_xgb = {\n                    'objective' : 'multi:softprob',\n                    'eval_metric' : 'mlogloss',\n                    'tree_method' : 'gpu_hist',\n                    'use_label_encoder': False,\n                    'n_estimators': N_ESTIMATORS,\n                    'random_state': seed\n                 }\n        param_cat = {\n                    'loss_function' : 'MultiClass', # MultiClassOneVsAll\n                    'eval_metric': 'MultiClass',\n                    'task_type' : 'GPU',            \n                    'n_estimators': N_ESTIMATORS,\n                    'random_state': seed,\n                    'verbose': VERBOSE\n                 }\n        param_lgb = {\n                    'objective' : 'multiclass',\n                    'n_estimators': N_ESTIMATORS,\n                    'device' : 'gpu',\n                    'random_state': seed\n                 }\n    else: #CPU\n        param_xgb = {\n                    'objective' : 'multi:softprob',\n                    'eval_metric' : 'mlogloss',\n                    'tree_method' : 'hist',\n                    'use_label_encoder': False,\n                    'n_estimators': N_ESTIMATORS,\n                    'random_state': seed\n                 }\n        param_cat = {\n                    'loss_function' : 'MultiClass',\n                    'eval_metric': 'MultiClass',\n                    'n_estimators': N_ESTIMATORS,\n                    'random_state': seed,\n                    'verbose': VERBOSE\n                 }\n        param_lgb = {\n                    'objective' : 'multiclass',\n                    'n_estimators': N_ESTIMATORS,\n                    'random_state': seed\n                 }\n        \n    if PRODUCTION:\n        models = [\n                    ExtraTreesClassifier(\n                        n_estimators=N_ESTIMATORS,\n                        random_state=seed,\n                        verbose=VERBOSE\n                    ),\n                   ]\n    else: # test run\n        models = [\n                    ExtraTreesClassifier(\n                        n_estimators=N_ESTIMATORS,\n                        random_state=seed,\n                        verbose=VERBOSE\n                    ),\n#                     XGBClassifier(**param_xgb),\n#                     lgb.LGBMClassifier(**param_lgb),\n#                     CatBoostClassifier(**param_cat),\n                   ]\n    for model in models:\n        model_name = type(model).__name__\n        print(f'******************Stacking {model_name:>20}*************************')\n        estimator_stack.append((f'{model_name}', model))\n        \n    return estimator_stack","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T02:39:06.000410Z","iopub.execute_input":"2022-02-08T02:39:06.000748Z","iopub.status.idle":"2022-02-08T02:39:06.017495Z","shell.execute_reply.started":"2022-02-08T02:39:06.000719Z","shell.execute_reply":"2022-02-08T02:39:06.016773Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"markdown","source":"## Stacker pipeline with CV\nDecision tree does not require feature scaling.\n\n**sklearn.linear_model.LogisticRegressionCV**\n\nclass_weight: dict or ‘balanced’, default=None\n\n    Weights associated with classes in the form {class_label: weight}. If not given, all classes are supposed to have weight one.\n\n    The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)).\n\n    Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified.","metadata":{}},{"cell_type":"code","source":"def build_stacking_classifier(estimator_stack, weights=[1.]*FOLDS, seed=RANDOM_STATE):\n    \n    # X pipeline StandardScaler MinMaxScaler RobustScaler , class_weight='balanced', Cs= \n    stacking_classifier = make_pipeline(\n#         StackingClassifier(estimators=estimator_stack, final_estimator=LogisticRegressionCV(multi_class='multinomial', max_iter=10000, cv=FOLDS, random_state=seed), cv=FOLDS, n_jobs=1, verbose=VERBOSE)\n        VotingClassifier(estimators=estimator_stack, voting='soft', weights=weights, n_jobs=1, verbose=VERBOSE)\n    )\n    return stacking_classifier","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T02:39:06.019198Z","iopub.execute_input":"2022-02-08T02:39:06.019571Z","iopub.status.idle":"2022-02-08T02:39:06.033280Z","shell.execute_reply.started":"2022-02-08T02:39:06.019522Z","shell.execute_reply":"2022-02-08T02:39:06.032253Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"markdown","source":"# Train model","metadata":{}},{"cell_type":"markdown","source":"Sample weight","metadata":{}},{"cell_type":"code","source":"np.unique(sample_weight, return_counts=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T02:51:54.700010Z","iopub.execute_input":"2022-02-08T02:51:54.700311Z","iopub.status.idle":"2022-02-08T02:51:54.709690Z","shell.execute_reply.started":"2022-02-08T02:51:54.700280Z","shell.execute_reply":"2022-02-08T02:51:54.708937Z"},"trusted":true},"execution_count":142,"outputs":[]},{"cell_type":"markdown","source":"## Test prediction","metadata":{}},{"cell_type":"code","source":"%%time\n\nmax_valid = (len(y_cat) // FOLDS) # Max consistent validation shape for dot product\n# Reset\nestimator_stack = []\nscores = []\npreds_test = []\npreds_valid_f = {}\npredictions = []\n\nfolds = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=RANDOM_STATE)\nfor fold, (train_id, test_id) in enumerate(folds.split(X, y_cat)):  \n    X_train = X.iloc[train_id]\n    y_train = y_cat[train_id]\n    X_valid = X.iloc[test_id]\n    y_valid = y_cat[test_id]\n    \n    # Build model\n    eclf = ExtraTreesClassifier(\n        n_estimators=N_ESTIMATORS,\n        class_weight='balanced',\n        random_state=RANDOM_STATE + fold,\n        verbose=VERBOSE\n    )\n    \n    # Train\n    eclf = eclf.fit(X_train, y_train, sample_weight=sample_weight[train_id])\n    # Validation\n    preds_valid = eclf.predict_proba(X_valid)\n    score_valid = accuracy_score(y_valid, np.argmax(preds_valid, axis=1))\n    \n    predictions.append([y_valid[:max_valid], preds_valid[:max_valid]])\n    index_valid = X_valid.index.tolist()\n    preds_valid_f.update(dict(zip(index_valid, le.inverse_transform(np.argmax(preds_valid, axis=1)))))\n    \n    print(\"Fold:\", fold + 1, \"Accuracy:\", score_valid, \"logloss:\", log_loss(y_valid, preds_valid), \"valid size:\", len(y_valid))\n\n    scores.append(score_valid)\n    preds_test.append(eclf.predict_proba(X_submission))\n    #     estimator_stack.append(eclf)\n    del eclf\n    gc.collect()\n\nplt.boxplot(scores, showmeans=True)\nplt.show()\nprint(\"Mean accuracy score:\", np.array(scores).mean())","metadata":{"execution":{"iopub.status.busy":"2022-02-08T02:40:34.237604Z","iopub.execute_input":"2022-02-08T02:40:34.237883Z","iopub.status.idle":"2022-02-08T02:49:38.242225Z","shell.execute_reply.started":"2022-02-08T02:40:34.237839Z","shell.execute_reply":"2022-02-08T02:49:38.241248Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"markdown","source":"## Validation Score","metadata":{}},{"cell_type":"markdown","source":"But instead of just looking at the mean accuracy across the 10 cross-validation folds, let's plot all 10 scores for each model, along with a box plot highlighting the lower and upper quartiles, and \"whiskers\" showing the extent of the scores. Note that the `boxplot()` function detects outliers (called \"fliers\") and does not include them within the whiskers. Specifically, if the lower quartile is $Q_1$ and the upper quartile is $Q_3$, then the interquartile range $IQR = Q_3 - Q_1$ (this is the box's height), and any score lower than $Q_1 - 1.5 \\times IQR$ is a flier, and so is any score greater than $Q3 + 1.5 \\times IQR$.","metadata":{}},{"cell_type":"code","source":"# %%time\n# if PRODUCTION:\n#     X_train, X_test, y_train, y_test = train_test_split(X, y_cat, stratify=y_cat, test_size=0.0001, random_state=RANDOM_STATE)\n# else: # test run\n#     X_train, X_test, y_train, y_test = train_test_split(X, y_cat, stratify=y_cat, test_size=VALIDATION_SPLIT, random_state=RANDOM_STATE)\n# # Reset\n# estimator_stack = []\n\n# # Build model\n# estimator_stack = build_estimator_stack(estimator_stack)\n# eclf = build_stacking_classifier(estimator_stack=estimator_stack, seed=RANDOM_STATE)\n\n\n# # Train\n# eclf = eclf.fit(X_train, y_train)\n# preds_test=[]\n# # Predict test dataset\n# preds_test.append(eclf.predict_proba(X_submission.values))\n# # resets\n# preds_valid_f = {}\n# index_valid = X_test.index.tolist()\n# preds_valid = eclf.predict_proba(X_test)\n# preds_valid_f.update(dict(zip(index_valid, le.inverse_transform(np.argmax(preds_valid, axis=1)))))\n# accuracy_score(y_test, np.argmax(preds_valid, axis=1))\n\n# log_loss(y_test, preds_valid)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T02:49:38.244774Z","iopub.execute_input":"2022-02-08T02:49:38.245377Z","iopub.status.idle":"2022-02-08T02:49:38.251219Z","shell.execute_reply.started":"2022-02-08T02:49:38.245322Z","shell.execute_reply":"2022-02-08T02:49:38.250191Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"log_loss_scores = []\naccuracy_scores = []\ndef log_loss_func(weights, verbose=VERBOSE):\n    ''' scipy minimize will pass the weights as a numpy array '''\n    final_prediction = 0\n    for weight, prediction in zip(weights, predictions):\n        final_prediction += weight*prediction[1]\n    \n    if verbose > 0:\n        print(f'log_loss: {log_loss(prediction[0], final_prediction)}')\n        \n    log_loss_score = log_loss(prediction[0], final_prediction)\n    log_loss_scores.append(log_loss_score)\n    return log_loss_score","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T02:49:38.252578Z","iopub.execute_input":"2022-02-08T02:49:38.252949Z","iopub.status.idle":"2022-02-08T02:49:38.272721Z","shell.execute_reply.started":"2022-02-08T02:49:38.252906Z","shell.execute_reply":"2022-02-08T02:49:38.271804Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"markdown","source":"## Average Weight Folds\n\nLets see if average weight folds across fold validation translate to test dataset as well.\nDid not work well. TBA","metadata":{}},{"cell_type":"code","source":"from scipy.optimize import minimize\n\n\n#the algorithms need a starting value, right not we chose 0.5 for all weights\n#its better to choose many random starting points and run minimize a few times\nstarting_values = [0.5]*len(predictions)\n#adding constraints  and a different solver as suggested by user 16universe\n#https://kaggle2.blob.core.windows.net/forum-message-attachments/75655/2393/otto%20model%20weights.pdf?sv=2012-02-12&se=2015-05-03T21%3A22%3A17Z&sr=b&sp=r&sig=rkeA7EJC%2BiQ%2FJ%2BcMpcA4lYQLFh6ubNqs2XAkGtFsAv0%3D\ncons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n#our weights are bound between 0 and 1\nbounds = [(1/(FOLDS+FOLDS*0.02),1)]*len(predictions)\n\nres = minimize(log_loss_func, starting_values, method='SLSQP', bounds=bounds, constraints=cons)\nprint('\\nEnsemble Score: {best_score}'.format(best_score=res['fun']))\nprint('Best Weights: {weights}'.format(weights=res['x']))\nplt.plot(log_loss_scores)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T02:49:38.274357Z","iopub.execute_input":"2022-02-08T02:49:38.274854Z","iopub.status.idle":"2022-02-08T02:49:38.638045Z","shell.execute_reply.started":"2022-02-08T02:49:38.274809Z","shell.execute_reply":"2022-02-08T02:49:38.637192Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"markdown","source":"Logistic Loss optimization plot","metadata":{}},{"cell_type":"code","source":"# Build model\n# eclf = build_stacking_classifier(estimator_stack=estimator_stack, weights=res['x'], seed=RANDOM_STATE)\n# preds_test=[]\n# Predict test dataset\n# preds_test.append(eclf.predict_proba(X_submission.values))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T02:49:38.640305Z","iopub.execute_input":"2022-02-08T02:49:38.640642Z","iopub.status.idle":"2022-02-08T02:49:38.644835Z","shell.execute_reply.started":"2022-02-08T02:49:38.640609Z","shell.execute_reply":"2022-02-08T02:49:38.643758Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"def log_loss_prediction(weights):\n    ''' scipy minimize will pass the weights as a numpy array '''\n    final_prediction = 0\n    for weight, prediction in zip(weights, preds_test):\n        final_prediction += weight*prediction\n    return final_prediction","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T02:49:38.646022Z","iopub.execute_input":"2022-02-08T02:49:38.646238Z","iopub.status.idle":"2022-02-08T02:49:38.656718Z","shell.execute_reply.started":"2022-02-08T02:49:38.646211Z","shell.execute_reply":"2022-02-08T02:49:38.656089Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"if AVERAGE_WEIGHTED_FOLD:\n    preds_final=[]\n    preds_final.append(log_loss_prediction(res['x']))\nelse:\n    preds_final=preds_test","metadata":{"execution":{"iopub.status.busy":"2022-02-08T02:49:38.658240Z","iopub.execute_input":"2022-02-08T02:49:38.658939Z","iopub.status.idle":"2022-02-08T02:49:38.670271Z","shell.execute_reply.started":"2022-02-08T02:49:38.658881Z","shell.execute_reply":"2022-02-08T02:49:38.669460Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"# if not PRODUCTION:\n#     scores = []\n#     for f in range(1,20,1):\n#         y_proba = preds_valid.copy()\n#         weight = np.array([0, 0, 0.01, f/100, 0, 0, 0, 0, 0, 0])\n#         y_proba += weight\n#         score = accuracy_score(y_test, np.argmax(y_proba, axis=1))\n#         scores.append(score)\n#         print(f'Score: {score}')\n\n#     max_score = max(scores)\n#     max_index = scores.index(max_score)\n#     print(f'\\nmax_score: {max_score} max_index: {max_index}')\n    \n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T02:49:38.671452Z","iopub.execute_input":"2022-02-08T02:49:38.671999Z","iopub.status.idle":"2022-02-08T02:49:38.681294Z","shell.execute_reply.started":"2022-02-08T02:49:38.671952Z","shell.execute_reply":"2022-02-08T02:49:38.680604Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"code","source":"oof_y_hat = []\nfor key, value in sorted(preds_valid_f.items()):\n    oof_y_hat.append(value)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T02:49:38.682634Z","iopub.execute_input":"2022-02-08T02:49:38.682927Z","iopub.status.idle":"2022-02-08T02:49:38.749243Z","shell.execute_reply.started":"2022-02-08T02:49:38.682884Z","shell.execute_reply":"2022-02-08T02:49:38.748581Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"markdown","source":"## Confusion matrix\n\n$Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}$\n\n$Precision = \\frac{TP}{TP+FP}$\n\n$Recall = \\frac{TP}{TP+FN}$\n\n$F1 = \\frac{2*Precision*Recall}{Precision+Recall} = \\frac{2*TP}{2*TP+FP+FN}$\n\nTODO: The gap is too huge. The network is overconfidence over some error.","metadata":{}},{"cell_type":"code","source":"# create confusion matrix, calculate accuracy,recall & precision\ncm = pd.DataFrame(data = confusion_matrix(le.inverse_transform(y_cat), oof_y_hat, labels = le.classes_), index = le.classes_, columns = le.classes_)\nplot_cm(cm)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T02:49:38.750577Z","iopub.execute_input":"2022-02-08T02:49:38.750814Z","iopub.status.idle":"2022-02-08T02:49:42.746304Z","shell.execute_reply.started":"2022-02-08T02:49:38.750783Z","shell.execute_reply":"2022-02-08T02:49:42.745815Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"code","source":"plot_cm_error(confusion_matrix(le.inverse_transform(y_cat), oof_y_hat, labels = le.classes_))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T02:49:42.747296Z","iopub.execute_input":"2022-02-08T02:49:42.747616Z","iopub.status.idle":"2022-02-08T02:49:44.263802Z","shell.execute_reply.started":"2022-02-08T02:49:42.747584Z","shell.execute_reply":"2022-02-08T02:49:44.263155Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"markdown","source":"# Postprocessing\nMean of Folds predictions","metadata":{}},{"cell_type":"code","source":"y_prob = sum(preds_final) / len(preds_final)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T02:49:44.264792Z","iopub.execute_input":"2022-02-08T02:49:44.265499Z","iopub.status.idle":"2022-02-08T02:49:44.278317Z","shell.execute_reply.started":"2022-02-08T02:49:44.265463Z","shell.execute_reply":"2022-02-08T02:49:44.277289Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"markdown","source":"# Weighted Average using distribution\nhttps://www.kaggle.com/ambrosm/tpsfeb22-02-postprocessing-against-the-mutants\n\nThere are differences in error bias between train dataset and test dataset. This is due to data compression and data loss.\n\nHypothesis:\n- The distribution of train dataset and test dataset remain the same.\n\nTODO: The equalizer function","metadata":{}},{"cell_type":"markdown","source":"## Distribution Before:","metadata":{}},{"cell_type":"code","source":"pd.Series(le.inverse_transform(np.argmax(y_prob, axis=1)), index=X_submission.index).value_counts().sort_index() / len(X_submission) * 100","metadata":{"execution":{"iopub.status.busy":"2022-02-08T02:49:44.279665Z","iopub.execute_input":"2022-02-08T02:49:44.280042Z","iopub.status.idle":"2022-02-08T02:49:44.308479Z","shell.execute_reply.started":"2022-02-08T02:49:44.280011Z","shell.execute_reply":"2022-02-08T02:49:44.307891Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"markdown","source":"## Distribution tuning","metadata":{}},{"cell_type":"code","source":"\n# Credit from https://www.kaggle.com/sfktrkl/tps-feb-2022\ndef get_diff(tune):\n    y_pred_tuned = np.argmax(y_prob + tune, axis=1)\n    return target_distribution - pd.Series(le.inverse_transform(y_pred_tuned)).value_counts().sort_index() / len(X_submission) * 100","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T02:49:44.311018Z","iopub.execute_input":"2022-02-08T02:49:44.311370Z","iopub.status.idle":"2022-02-08T02:49:44.315908Z","shell.execute_reply.started":"2022-02-08T02:49:44.311340Z","shell.execute_reply":"2022-02-08T02:49:44.315051Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"markdown","source":"## Distribution Target","metadata":{}},{"cell_type":"code","source":"target_distribution","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T02:49:44.317013Z","iopub.execute_input":"2022-02-08T02:49:44.317354Z","iopub.status.idle":"2022-02-08T02:49:44.330986Z","shell.execute_reply.started":"2022-02-08T02:49:44.317325Z","shell.execute_reply":"2022-02-08T02:49:44.330007Z"},"trusted":true},"execution_count":130,"outputs":[]},{"cell_type":"markdown","source":"## Initial Diff","metadata":{}},{"cell_type":"code","source":"tune = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\ndiff = get_diff(tune)\nprint(diff)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T02:49:44.332038Z","iopub.execute_input":"2022-02-08T02:49:44.332728Z","iopub.status.idle":"2022-02-08T02:49:44.367654Z","shell.execute_reply.started":"2022-02-08T02:49:44.332680Z","shell.execute_reply":"2022-02-08T02:49:44.367046Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"code","source":"%%time\n\nDIST_THRESHOLD = 0.1 # Distribution variance threshold\nMAX_ITERATION = 3500  # Max rounds\niterate = 0\n\nwhile (abs(diff).max() > DIST_THRESHOLD) and (iterate < MAX_ITERATION):\n    iterate += 1\n    for i in range(len(diff)):\n        if diff[i] > DIST_THRESHOLD:\n            tune[i] += 0.0003\n            break\n        if diff[i] < -DIST_THRESHOLD:\n            tune[i] -= 0.0003\n            break\n\n    diff = get_diff(tune)\n    \nprint(f'After {iterate} rounds')\nprint(f'Best weights: {tune}')\nprint(f'\\nRemaining difference:\\n{diff}')","metadata":{"execution":{"iopub.status.busy":"2022-02-08T02:49:44.368610Z","iopub.execute_input":"2022-02-08T02:49:44.369265Z","iopub.status.idle":"2022-02-08T02:49:50.355681Z","shell.execute_reply.started":"2022-02-08T02:49:44.369228Z","shell.execute_reply":"2022-02-08T02:49:50.354748Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":"def dist_diff_loss_func(weights):\n    diff = get_diff(weights)\n    return abs(diff).max()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T02:49:50.356925Z","iopub.execute_input":"2022-02-08T02:49:50.357133Z","iopub.status.idle":"2022-02-08T02:49:50.361299Z","shell.execute_reply.started":"2022-02-08T02:49:50.357107Z","shell.execute_reply":"2022-02-08T02:49:50.360333Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"y_prob += tune\ny_pred_tuned = le.inverse_transform(np.argmax(y_prob, axis=1))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T02:49:50.362947Z","iopub.execute_input":"2022-02-08T02:49:50.363322Z","iopub.status.idle":"2022-02-08T02:49:50.388478Z","shell.execute_reply.started":"2022-02-08T02:49:50.363249Z","shell.execute_reply":"2022-02-08T02:49:50.387472Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"markdown","source":"## Distribution After:","metadata":{}},{"cell_type":"code","source":"pd.Series(y_pred_tuned, index=X_submission.index).value_counts().sort_index() / len(X_submission) * 100","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T02:49:50.390244Z","iopub.execute_input":"2022-02-08T02:49:50.390534Z","iopub.status.idle":"2022-02-08T02:49:50.413048Z","shell.execute_reply.started":"2022-02-08T02:49:50.390501Z","shell.execute_reply":"2022-02-08T02:49:50.411937Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"sub = pd.read_csv(INPUT + '/sample_submission.csv')\nsub[TARGET_FEATURE_NAME] = y_pred_tuned\nsub.to_csv(\"submission.csv\", index=False)\nsub.to_csv(\"submission_00.csv\", index=False)\ndisplay(sub.head(10))\ndisplay(sub.tail(10))","metadata":{"execution":{"iopub.status.busy":"2022-02-08T02:49:50.414418Z","iopub.execute_input":"2022-02-08T02:49:50.415436Z","iopub.status.idle":"2022-02-08T02:49:51.018557Z","shell.execute_reply.started":"2022-02-08T02:49:50.415398Z","shell.execute_reply":"2022-02-08T02:49:51.017915Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"# Plot the distribution of the test predictions vs training set\nplt.figure(figsize=(20,6))\nplt.hist(y[TARGET_FEATURE_NAME], bins = np.linspace(0.5, 7.5, 8), density = True, label = 'Training labels')\nplt.hist(sub[TARGET_FEATURE_NAME], bins = np.linspace(0.5, 7.5, 8), density = True, rwidth = 0.7, label = 'Test predictions')\nplt.xlabel(TARGET_FEATURE_NAME)\nplt.ylabel('Frequency')\nplt.gca().yaxis.set_major_formatter(PercentFormatter())\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T02:49:51.019582Z","iopub.execute_input":"2022-02-08T02:49:51.020331Z","iopub.status.idle":"2022-02-08T02:49:51.417537Z","shell.execute_reply.started":"2022-02-08T02:49:51.020297Z","shell.execute_reply":"2022-02-08T02:49:51.416637Z"},"trusted":true},"execution_count":137,"outputs":[]},{"cell_type":"code","source":"def plot_x_labels(ax):\n    for rect in ax.patches:\n        height = rect.get_height()\n        ax.annotate(f'{int(height)}', xy=(rect.get_x()+rect.get_width()/2, height), \n                    xytext=(0, 5), textcoords='offset points', ha='center', va='bottom') \n\n# Plot the distribution of the test predictions\nfig, ax = plt.subplots(2,1,figsize = (20,10))\nsns.countplot(x = sub[TARGET_FEATURE_NAME], ax = ax[0], orient = \"h\").set_title(\"Prediction\")\nplot_x_labels(ax[0])\n# Plot the distribution of the training set\nsns.countplot(x = y[TARGET_FEATURE_NAME], ax = ax[1], orient = \"h\").set_title(\"Training labels\")\nplot_x_labels(ax[1])\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T02:49:51.418834Z","iopub.execute_input":"2022-02-08T02:49:51.419098Z","iopub.status.idle":"2022-02-08T02:49:52.530917Z","shell.execute_reply.started":"2022-02-08T02:49:51.419068Z","shell.execute_reply":"2022-02-08T02:49:52.530252Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"markdown","source":"To submit these predictions to the competition, follow these steps:\n\n1. Begin by clicking on the blue **Save Version** button in the top right corner of the window.  This will generate a pop-up window.\n2. Ensure that the **Save and Run All** option is selected, and then click on the blue **Save** button.\n3. This generates a window in the bottom left corner of the notebook.  After it has finished running, click on the number to the right of the **Save Version** button.  This pulls up a list of versions on the right of the screen.  Click on the ellipsis **(...)** to the right of the most recent version, and select **Open in Viewer**.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n4. Click on the **Output** tab on the right of the screen.  Then, click on the file you would like to submit, and click on the blue **Submit** button to submit your results to the leaderboard.\n\nYou have now successfully submitted to the competition!\n\n# Next Steps #\n\nIf you want to keep working to improve your performance, select the blue **Edit** button in the top right of the screen. Then you can change your code and repeat the process. There's a lot of room to improve, and you will climb up the leaderboard as you work.\n\nBe sure to check out [other users' notebooks](https://www.kaggle.com/c/tabular-playground-series-feb-2022/code) in this competition. You'll find lots of great ideas for new features and as well as other ways to discover more things about the dataset or make better predictions. There's also the [discussion forum](https://www.kaggle.com/c/tabular-playground-series-feb-2022/discussion), where you can share ideas with other Kagglers.\n\nHave fun Kaggling!","metadata":{}}]}