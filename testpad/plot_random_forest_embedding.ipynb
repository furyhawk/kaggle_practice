{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Hashing feature transformation using Totally Random Trees\n",
        "\n",
        "RandomTreesEmbedding provides a way to map data to a\n",
        "very high-dimensional, sparse representation, which might\n",
        "be beneficial for classification.\n",
        "The mapping is completely unsupervised and very efficient.\n",
        "\n",
        "This example visualizes the partitions given by several\n",
        "trees and shows how the transformation can also be used for\n",
        "non-linear dimensionality reduction or non-linear classification.\n",
        "\n",
        "Points that are neighboring often share the same leaf of a tree and therefore\n",
        "share large parts of their hashed representation. This allows to\n",
        "separate two concentric circles simply based on the principal components\n",
        "of the transformed data with truncated SVD.\n",
        "\n",
        "In high-dimensional spaces, linear classifiers often achieve\n",
        "excellent accuracy. For sparse binary data, BernoulliNB\n",
        "is particularly well-suited. The bottom row compares the\n",
        "decision boundary obtained by BernoulliNB in the transformed\n",
        "space with an ExtraTreesClassifier forests learned on the\n",
        "original data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.ensemble import RandomTreesEmbedding, ExtraTreesClassifier\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "# make a synthetic dataset\n",
        "X, y = make_circles(factor=0.5, random_state=0, noise=0.05)\n",
        "\n",
        "# use RandomTreesEmbedding to transform data\n",
        "hasher = RandomTreesEmbedding(n_estimators=10, random_state=0, max_depth=3)\n",
        "X_transformed = hasher.fit_transform(X)\n",
        "\n",
        "# Visualize result after dimensionality reduction using truncated SVD\n",
        "svd = TruncatedSVD(n_components=2)\n",
        "X_reduced = svd.fit_transform(X_transformed)\n",
        "\n",
        "# Learn a Naive Bayes classifier on the transformed data\n",
        "nb = BernoulliNB()\n",
        "nb.fit(X_transformed, y)\n",
        "\n",
        "\n",
        "# Learn an ExtraTreesClassifier for comparison\n",
        "trees = ExtraTreesClassifier(max_depth=3, n_estimators=10, random_state=0)\n",
        "trees.fit(X, y)\n",
        "\n",
        "\n",
        "# scatter plot of original and reduced data\n",
        "fig = plt.figure(figsize=(9, 8))\n",
        "\n",
        "ax = plt.subplot(221)\n",
        "ax.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolor=\"k\")\n",
        "ax.set_title(\"Original Data (2d)\")\n",
        "ax.set_xticks(())\n",
        "ax.set_yticks(())\n",
        "\n",
        "ax = plt.subplot(222)\n",
        "ax.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, s=50, edgecolor=\"k\")\n",
        "ax.set_title(\n",
        "    \"Truncated SVD reduction (2d) of transformed data (%dd)\" % X_transformed.shape[1]\n",
        ")\n",
        "ax.set_xticks(())\n",
        "ax.set_yticks(())\n",
        "\n",
        "# Plot the decision in original space. For that, we will assign a color\n",
        "# to each point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "h = 0.01\n",
        "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
        "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "\n",
        "# transform grid using RandomTreesEmbedding\n",
        "transformed_grid = hasher.transform(np.c_[xx.ravel(), yy.ravel()])\n",
        "y_grid_pred = nb.predict_proba(transformed_grid)[:, 1]\n",
        "\n",
        "ax = plt.subplot(223)\n",
        "ax.set_title(\"Naive Bayes on Transformed data\")\n",
        "ax.pcolormesh(xx, yy, y_grid_pred.reshape(xx.shape))\n",
        "ax.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolor=\"k\")\n",
        "ax.set_ylim(-1.4, 1.4)\n",
        "ax.set_xlim(-1.4, 1.4)\n",
        "ax.set_xticks(())\n",
        "ax.set_yticks(())\n",
        "\n",
        "# transform grid using ExtraTreesClassifier\n",
        "y_grid_pred = trees.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
        "\n",
        "ax = plt.subplot(224)\n",
        "ax.set_title(\"ExtraTrees predictions\")\n",
        "ax.pcolormesh(xx, yy, y_grid_pred.reshape(xx.shape))\n",
        "ax.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolor=\"k\")\n",
        "ax.set_ylim(-1.4, 1.4)\n",
        "ax.set_xlim(-1.4, 1.4)\n",
        "ax.set_xticks(())\n",
        "ax.set_yticks(())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
