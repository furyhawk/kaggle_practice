{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "X_test, y_test = helpers.get_data(subset=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.OrdinalEncoder()\n",
    "le.fit(X_test['native-country'].astype(str).values.reshape(-1,1))\n",
    "le.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "X_test, y_test = helpers.get_data(subset=\"test\")\n",
    "ohe = preprocessing.OneHotEncoder()\n",
    "categories = np.array(list(set(X_test['workclass'].astype(str).values))).reshape(-1,1)\n",
    "ohe.fit(categories)\n",
    "\n",
    "# OneHotEncoder(categorical_features=None, categories=None,\n",
    "#     dtype=<class 'numpy.float64'>, handle_unknown='error',\n",
    "#     n_values=None, sparse=True)\n",
    "\n",
    "categories\n",
    "\n",
    "# array([['Self-emp-inc'],\n",
    "#     ['Local-gov'],\n",
    "#     ['Private'],\n",
    "#     ['State-gov'],\n",
    "#     ['Never-worked'],\n",
    "#     ['Without-pay'],\n",
    "#     ['Federal-gov'],\n",
    "#     ['Self-emp-not-inc'],\n",
    "#     ['nan']], dtype='<U16')\n",
    "ohe.transform(categories).todense()\n",
    "\n",
    "# matrix([[0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
    "#     [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "#     [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
    "#     [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
    "#     [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
    "#     [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
    "#     [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "#     [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
    "#     [0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_encoder_pp = helpers.to_pipeline((\"ohe\", preprocessing.OneHotEncoder(handle_unknown='ignore', categories=categories.get_categories())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_pipeline = pipeline.get_pipeline(ohe_encoder_pp, input_dim=112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.execute_on_pipeline(ohe_pipeline, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers.core import Dense, Reshape, Lambda\n",
    "from keras.layers import Input, Embedding, merge\n",
    "from keras import backend as K\n",
    "\n",
    "# Number of product IDs available\n",
    "N_products = 1000000\n",
    "N_stores = 1000\n",
    "N_shoppers = 10000\n",
    "\n",
    "# Integer IDs representing 1-hot encodings\n",
    "prior_in = Input(shape=(1,))\n",
    "store_in = Input(shape=(1,))\n",
    "shopper_in = Input(shape=(1,))\n",
    "\n",
    "# Dense N-hot encoding for candidate products\n",
    "candidates_in = Input(shape=(N_products,))\n",
    "\n",
    "# Embeddings\n",
    "prior = Embedding(N_products, 10)(prior_in)\n",
    "store = Embedding(N_stores, 10)(store_in)\n",
    "shopper = Embedding(N_shoppers, 10)(shopper_in)\n",
    "\n",
    "# Reshape and merge all embeddings together\n",
    "reshape = Reshape(target_shape=(10,))\n",
    "combined = merge([reshape(prior), reshape(store), reshape(shopper)],\n",
    "                 mode='concat')\n",
    "\n",
    "# Hidden layers\n",
    "hidden_1 = Dense(1024, activation='relu')(combined)\n",
    "hidden_2 = Dense(512, activation='relu')(hidden_1)\n",
    "hidden_3 = Dense(256, activation='relu')(hidden_2)\n",
    "hidden_4 = Dense(10, activation='linear')(hidden_3)\n",
    "\n",
    "# Final 'fan-out' into the space of future products\n",
    "final = Dense(N_products, activation='linear')(hidden_4)\n",
    "\n",
    "# Ensure we do not overflow when we exponentiate\n",
    "final = Lambda(lambda x: x - K.max(x))(final)\n",
    "\n",
    "# Masked soft-max using Lambda and merge-multiplication\n",
    "exponentiate = Lambda(lambda x: K.exp(x))(final)\n",
    "masked = merge([exponentiate, candidates_in], mode='mul')\n",
    "predicted = Lambda(lambda x: x / K.sum(x))(masked)\n",
    "\n",
    "# Compile with categorical crossentropy and adam\n",
    "mdl = Model(input=[prior_in, store_in, shopper_in, candidates_in],\n",
    "            output=predicted)\n",
    "mdl.compile(loss='categorical_crossentropy', \n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\furyx\\miniconda3\\envs\\tf26\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:98: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\furyx\\miniconda3\\envs\\tf26\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12/12 - 0s - loss: 0.9597 - accuracy: 0.7068\n",
      "Epoch 2/100\n",
      "12/12 - 0s - loss: 0.9112 - accuracy: 0.6963\n",
      "Epoch 3/100\n",
      "12/12 - 0s - loss: 0.8798 - accuracy: 0.7068\n",
      "Epoch 4/100\n",
      "12/12 - 0s - loss: 0.8518 - accuracy: 0.6963\n",
      "Epoch 5/100\n",
      "12/12 - 0s - loss: 0.8313 - accuracy: 0.7016\n",
      "Epoch 6/100\n",
      "12/12 - 0s - loss: 0.8140 - accuracy: 0.7016\n",
      "Epoch 7/100\n",
      "12/12 - 0s - loss: 0.7977 - accuracy: 0.7016\n",
      "Epoch 8/100\n",
      "12/12 - 0s - loss: 0.7836 - accuracy: 0.7016\n",
      "Epoch 9/100\n",
      "12/12 - 0s - loss: 0.7684 - accuracy: 0.7068\n",
      "Epoch 10/100\n",
      "12/12 - 0s - loss: 0.7567 - accuracy: 0.7016\n",
      "Epoch 11/100\n",
      "12/12 - 0s - loss: 0.7449 - accuracy: 0.7016\n",
      "Epoch 12/100\n",
      "12/12 - 0s - loss: 0.7331 - accuracy: 0.7068\n",
      "Epoch 13/100\n",
      "12/12 - 0s - loss: 0.7230 - accuracy: 0.7016\n",
      "Epoch 14/100\n",
      "12/12 - 0s - loss: 0.7134 - accuracy: 0.6963\n",
      "Epoch 15/100\n",
      "12/12 - 0s - loss: 0.7043 - accuracy: 0.7016\n",
      "Epoch 16/100\n",
      "12/12 - 0s - loss: 0.6952 - accuracy: 0.7068\n",
      "Epoch 17/100\n",
      "12/12 - 0s - loss: 0.6866 - accuracy: 0.7016\n",
      "Epoch 18/100\n",
      "12/12 - 0s - loss: 0.6792 - accuracy: 0.7016\n",
      "Epoch 19/100\n",
      "12/12 - 0s - loss: 0.6724 - accuracy: 0.7016\n",
      "Epoch 20/100\n",
      "12/12 - 0s - loss: 0.6656 - accuracy: 0.7016\n",
      "Epoch 21/100\n",
      "12/12 - 0s - loss: 0.6598 - accuracy: 0.6963\n",
      "Epoch 22/100\n",
      "12/12 - 0s - loss: 0.6551 - accuracy: 0.7068\n",
      "Epoch 23/100\n",
      "12/12 - 0s - loss: 0.6490 - accuracy: 0.7016\n",
      "Epoch 24/100\n",
      "12/12 - 0s - loss: 0.6440 - accuracy: 0.7068\n",
      "Epoch 25/100\n",
      "12/12 - 0s - loss: 0.6376 - accuracy: 0.7068\n",
      "Epoch 26/100\n",
      "12/12 - 0s - loss: 0.6332 - accuracy: 0.7068\n",
      "Epoch 27/100\n",
      "12/12 - 0s - loss: 0.6284 - accuracy: 0.7068\n",
      "Epoch 28/100\n",
      "12/12 - 0s - loss: 0.6263 - accuracy: 0.7068\n",
      "Epoch 29/100\n",
      "12/12 - 0s - loss: 0.6203 - accuracy: 0.7068\n",
      "Epoch 30/100\n",
      "12/12 - 0s - loss: 0.6163 - accuracy: 0.7016\n",
      "Epoch 31/100\n",
      "12/12 - 0s - loss: 0.6134 - accuracy: 0.7016\n",
      "Epoch 32/100\n",
      "12/12 - 0s - loss: 0.6090 - accuracy: 0.7016\n",
      "Epoch 33/100\n",
      "12/12 - 0s - loss: 0.6049 - accuracy: 0.7016\n",
      "Epoch 34/100\n",
      "12/12 - 0s - loss: 0.6028 - accuracy: 0.7120\n",
      "Epoch 35/100\n",
      "12/12 - 0s - loss: 0.5984 - accuracy: 0.7068\n",
      "Epoch 36/100\n",
      "12/12 - 0s - loss: 0.5951 - accuracy: 0.7068\n",
      "Epoch 37/100\n",
      "12/12 - 0s - loss: 0.5934 - accuracy: 0.7068\n",
      "Epoch 38/100\n",
      "12/12 - 0s - loss: 0.5889 - accuracy: 0.7120\n",
      "Epoch 39/100\n",
      "12/12 - 0s - loss: 0.5867 - accuracy: 0.7068\n",
      "Epoch 40/100\n",
      "12/12 - 0s - loss: 0.5831 - accuracy: 0.7173\n",
      "Epoch 41/100\n",
      "12/12 - 0s - loss: 0.5805 - accuracy: 0.7016\n",
      "Epoch 42/100\n",
      "12/12 - 0s - loss: 0.5785 - accuracy: 0.7068\n",
      "Epoch 43/100\n",
      "12/12 - 0s - loss: 0.5758 - accuracy: 0.7068\n",
      "Epoch 44/100\n",
      "12/12 - 0s - loss: 0.5741 - accuracy: 0.7173\n",
      "Epoch 45/100\n",
      "12/12 - 0s - loss: 0.5720 - accuracy: 0.7225\n",
      "Epoch 46/100\n",
      "12/12 - 0s - loss: 0.5689 - accuracy: 0.7225\n",
      "Epoch 47/100\n",
      "12/12 - 0s - loss: 0.5667 - accuracy: 0.7330\n",
      "Epoch 48/100\n",
      "12/12 - 0s - loss: 0.5664 - accuracy: 0.7225\n",
      "Epoch 49/100\n",
      "12/12 - 0s - loss: 0.5627 - accuracy: 0.7277\n",
      "Epoch 50/100\n",
      "12/12 - 0s - loss: 0.5612 - accuracy: 0.7487\n",
      "Epoch 51/100\n",
      "12/12 - 0s - loss: 0.5606 - accuracy: 0.7435\n",
      "Epoch 52/100\n",
      "12/12 - 0s - loss: 0.5573 - accuracy: 0.7382\n",
      "Epoch 53/100\n",
      "12/12 - 0s - loss: 0.5569 - accuracy: 0.7487\n",
      "Epoch 54/100\n",
      "12/12 - 0s - loss: 0.5544 - accuracy: 0.7435\n",
      "Epoch 55/100\n",
      "12/12 - 0s - loss: 0.5529 - accuracy: 0.7435\n",
      "Epoch 56/100\n",
      "12/12 - 0s - loss: 0.5513 - accuracy: 0.7487\n",
      "Epoch 57/100\n",
      "12/12 - 0s - loss: 0.5499 - accuracy: 0.7487\n",
      "Epoch 58/100\n",
      "12/12 - 0s - loss: 0.5491 - accuracy: 0.7435\n",
      "Epoch 59/100\n",
      "12/12 - 0s - loss: 0.5486 - accuracy: 0.7487\n",
      "Epoch 60/100\n",
      "12/12 - 0s - loss: 0.5474 - accuracy: 0.7487\n",
      "Epoch 61/100\n",
      "12/12 - 0s - loss: 0.5454 - accuracy: 0.7592\n",
      "Epoch 62/100\n",
      "12/12 - 0s - loss: 0.5440 - accuracy: 0.7644\n",
      "Epoch 63/100\n",
      "12/12 - 0s - loss: 0.5429 - accuracy: 0.7539\n",
      "Epoch 64/100\n",
      "12/12 - 0s - loss: 0.5418 - accuracy: 0.7696\n",
      "Epoch 65/100\n",
      "12/12 - 0s - loss: 0.5404 - accuracy: 0.7696\n",
      "Epoch 66/100\n",
      "12/12 - 0s - loss: 0.5395 - accuracy: 0.7644\n",
      "Epoch 67/100\n",
      "12/12 - 0s - loss: 0.5387 - accuracy: 0.7749\n",
      "Epoch 68/100\n",
      "12/12 - 0s - loss: 0.5376 - accuracy: 0.7749\n",
      "Epoch 69/100\n",
      "12/12 - 0s - loss: 0.5367 - accuracy: 0.7906\n",
      "Epoch 70/100\n",
      "12/12 - 0s - loss: 0.5361 - accuracy: 0.7801\n",
      "Epoch 71/100\n",
      "12/12 - 0s - loss: 0.5349 - accuracy: 0.7696\n",
      "Epoch 72/100\n",
      "12/12 - 0s - loss: 0.5346 - accuracy: 0.7749\n",
      "Epoch 73/100\n",
      "12/12 - 0s - loss: 0.5332 - accuracy: 0.7749\n",
      "Epoch 74/100\n",
      "12/12 - 0s - loss: 0.5331 - accuracy: 0.7853\n",
      "Epoch 75/100\n",
      "12/12 - 0s - loss: 0.5319 - accuracy: 0.7801\n",
      "Epoch 76/100\n",
      "12/12 - 0s - loss: 0.5314 - accuracy: 0.7906\n",
      "Epoch 77/100\n",
      "12/12 - 0s - loss: 0.5306 - accuracy: 0.7958\n",
      "Epoch 78/100\n",
      "12/12 - 0s - loss: 0.5291 - accuracy: 0.7853\n",
      "Epoch 79/100\n",
      "12/12 - 0s - loss: 0.5289 - accuracy: 0.7853\n",
      "Epoch 80/100\n",
      "12/12 - 0s - loss: 0.5280 - accuracy: 0.7958\n",
      "Epoch 81/100\n",
      "12/12 - 0s - loss: 0.5273 - accuracy: 0.7853\n",
      "Epoch 82/100\n",
      "12/12 - 0s - loss: 0.5264 - accuracy: 0.7853\n",
      "Epoch 83/100\n",
      "12/12 - 0s - loss: 0.5257 - accuracy: 0.7906\n",
      "Epoch 84/100\n",
      "12/12 - 0s - loss: 0.5254 - accuracy: 0.7906\n",
      "Epoch 85/100\n",
      "12/12 - 0s - loss: 0.5250 - accuracy: 0.7906\n",
      "Epoch 86/100\n",
      "12/12 - 0s - loss: 0.5244 - accuracy: 0.7853\n",
      "Epoch 87/100\n",
      "12/12 - 0s - loss: 0.5235 - accuracy: 0.7906\n",
      "Epoch 88/100\n",
      "12/12 - 0s - loss: 0.5226 - accuracy: 0.7853\n",
      "Epoch 89/100\n",
      "12/12 - 0s - loss: 0.5222 - accuracy: 0.7853\n",
      "Epoch 90/100\n",
      "12/12 - 0s - loss: 0.5250 - accuracy: 0.7853\n",
      "Epoch 91/100\n",
      "12/12 - 0s - loss: 0.5231 - accuracy: 0.7801\n",
      "Epoch 92/100\n",
      "12/12 - 0s - loss: 0.5222 - accuracy: 0.7801\n",
      "Epoch 93/100\n",
      "12/12 - 0s - loss: 0.5206 - accuracy: 0.7801\n",
      "Epoch 94/100\n",
      "12/12 - 0s - loss: 0.5197 - accuracy: 0.7853\n",
      "Epoch 95/100\n",
      "12/12 - 0s - loss: 0.5211 - accuracy: 0.7853\n",
      "Epoch 96/100\n",
      "12/12 - 0s - loss: 0.5189 - accuracy: 0.7853\n",
      "Epoch 97/100\n",
      "12/12 - 0s - loss: 0.5182 - accuracy: 0.7801\n",
      "Epoch 98/100\n",
      "12/12 - 0s - loss: 0.5190 - accuracy: 0.7801\n",
      "Epoch 99/100\n",
      "12/12 - 0s - loss: 0.5196 - accuracy: 0.7801\n",
      "Epoch 100/100\n",
      "12/12 - 0s - loss: 0.5189 - accuracy: 0.7801\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x0000022E14FE1C10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Accuracy: 73.68\n"
     ]
    }
   ],
   "source": [
    "# example of ordinal encoding for a neural network\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(filename):\n",
    "\t# load the dataset as a pandas DataFrame\n",
    "\tdata = read_csv(filename, header=None)\n",
    "\t# retrieve numpy array\n",
    "\tdataset = data.values\n",
    "\t# split into input (X) and output (y) variables\n",
    "\tX = dataset[:, :-1]\n",
    "\ty = dataset[:,-1]\n",
    "\t# format all fields as string\n",
    "\tX = X.astype(str)\n",
    "\t# reshape target to be a 2d array\n",
    "\ty = y.reshape((len(y), 1))\n",
    "\treturn X, y\n",
    "\n",
    "# prepare input data\n",
    "def prepare_inputs(X_train, X_test):\n",
    "\toe = OrdinalEncoder()\n",
    "\toe.fit(X_train)\n",
    "\tX_train_enc = oe.transform(X_train)\n",
    "\tX_test_enc = oe.transform(X_test)\n",
    "\treturn X_train_enc, X_test_enc\n",
    "\n",
    "# prepare target\n",
    "def prepare_targets(y_train, y_test):\n",
    "\tle = LabelEncoder()\n",
    "\tle.fit(y_train)\n",
    "\ty_train_enc = le.transform(y_train)\n",
    "\ty_test_enc = le.transform(y_test)\n",
    "\treturn y_train_enc, y_test_enc\n",
    "\n",
    "# load the dataset\n",
    "X, y = load_dataset('../input/breast-cancer.csv')\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# prepare input data\n",
    "X_train_enc, X_test_enc = prepare_inputs(X_train, X_test)\n",
    "# prepare output data\n",
    "y_train_enc, y_test_enc = prepare_targets(y_train, y_test)\n",
    "# define the  model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=X_train_enc.shape[1], activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X_train_enc, y_train_enc, epochs=100, batch_size=16, verbose=2)\n",
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(X_test_enc, y_test_enc, verbose=0)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to One Hot Encode Categorical Data\n",
    "\n",
    "A one hot encoding is appropriate for categorical data where no relationship exists between categories.\n",
    "\n",
    "It involves representing each categorical variable with a binary vector that has one element for each unique label and marking the class label with a 1 and all other elements 0.\n",
    "\n",
    "For example, if our variable was “color” and the labels were “red,” “green,” and “blue,” we would encode each of these labels as a three-element binary vector as follows:\n",
    "\n",
    "    Red: [1, 0, 0]\n",
    "    Green: [0, 1, 0]\n",
    "    Blue: [0, 0, 1]\n",
    "\n",
    "Then each label in the dataset would be replaced with a vector (one column becomes three). This is done for all categorical variables so that our nine input variables or columns become 43 in the case of the breast cancer dataset.\n",
    "\n",
    "The scikit-learn library provides the OneHotEncoder to automatically one hot encode one or more variables.\n",
    "\n",
    "The prepare_inputs() function below provides a drop-in replacement function for the example in the previous section. Instead of using an OrdinalEncoder, it uses a OneHotEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\furyx\\miniconda3\\envs\\tf26\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:98: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\furyx\\miniconda3\\envs\\tf26\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\furyx\\miniconda3\\envs\\tf26\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/sequential_2/dense_7/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/sequential_2/dense_7/embedding_lookup_sparse/Reshape:0\", shape=(None, 10), dtype=float32), dense_shape=Tensor(\"gradient_tape/sequential_2/dense_7/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12/12 - 0s - loss: 0.7343 - accuracy: 0.4607\n",
      "Epoch 2/100\n",
      "12/12 - 0s - loss: 0.7010 - accuracy: 0.5445\n",
      "Epoch 3/100\n",
      "12/12 - 0s - loss: 0.6718 - accuracy: 0.6073\n",
      "Epoch 4/100\n",
      "12/12 - 0s - loss: 0.6472 - accuracy: 0.6649\n",
      "Epoch 5/100\n",
      "12/12 - 0s - loss: 0.6268 - accuracy: 0.6963\n",
      "Epoch 6/100\n",
      "12/12 - 0s - loss: 0.6105 - accuracy: 0.7120\n",
      "Epoch 7/100\n",
      "12/12 - 0s - loss: 0.5960 - accuracy: 0.7277\n",
      "Epoch 8/100\n",
      "12/12 - 0s - loss: 0.5852 - accuracy: 0.7330\n",
      "Epoch 9/100\n",
      "12/12 - 0s - loss: 0.5749 - accuracy: 0.7382\n",
      "Epoch 10/100\n",
      "12/12 - 0s - loss: 0.5673 - accuracy: 0.7435\n",
      "Epoch 11/100\n",
      "12/12 - 0s - loss: 0.5596 - accuracy: 0.7435\n",
      "Epoch 12/100\n",
      "12/12 - 0s - loss: 0.5533 - accuracy: 0.7435\n",
      "Epoch 13/100\n",
      "12/12 - 0s - loss: 0.5472 - accuracy: 0.7435\n",
      "Epoch 14/100\n",
      "12/12 - 0s - loss: 0.5417 - accuracy: 0.7539\n",
      "Epoch 15/100\n",
      "12/12 - 0s - loss: 0.5370 - accuracy: 0.7539\n",
      "Epoch 16/100\n",
      "12/12 - 0s - loss: 0.5324 - accuracy: 0.7539\n",
      "Epoch 17/100\n",
      "12/12 - 0s - loss: 0.5278 - accuracy: 0.7592\n",
      "Epoch 18/100\n",
      "12/12 - 0s - loss: 0.5239 - accuracy: 0.7592\n",
      "Epoch 19/100\n",
      "12/12 - 0s - loss: 0.5202 - accuracy: 0.7592\n",
      "Epoch 20/100\n",
      "12/12 - 0s - loss: 0.5165 - accuracy: 0.7644\n",
      "Epoch 21/100\n",
      "12/12 - 0s - loss: 0.5129 - accuracy: 0.7644\n",
      "Epoch 22/100\n",
      "12/12 - 0s - loss: 0.5094 - accuracy: 0.7644\n",
      "Epoch 23/100\n",
      "12/12 - 0s - loss: 0.5064 - accuracy: 0.7749\n",
      "Epoch 24/100\n",
      "12/12 - 0s - loss: 0.5034 - accuracy: 0.7749\n",
      "Epoch 25/100\n",
      "12/12 - 0s - loss: 0.5005 - accuracy: 0.7749\n",
      "Epoch 26/100\n",
      "12/12 - 0s - loss: 0.4977 - accuracy: 0.7853\n",
      "Epoch 27/100\n",
      "12/12 - 0s - loss: 0.4944 - accuracy: 0.7906\n",
      "Epoch 28/100\n",
      "12/12 - 0s - loss: 0.4915 - accuracy: 0.7906\n",
      "Epoch 29/100\n",
      "12/12 - 0s - loss: 0.4880 - accuracy: 0.7906\n",
      "Epoch 30/100\n",
      "12/12 - 0s - loss: 0.4857 - accuracy: 0.7906\n",
      "Epoch 31/100\n",
      "12/12 - 0s - loss: 0.4822 - accuracy: 0.7906\n",
      "Epoch 32/100\n",
      "12/12 - 0s - loss: 0.4798 - accuracy: 0.7906\n",
      "Epoch 33/100\n",
      "12/12 - 0s - loss: 0.4771 - accuracy: 0.7906\n",
      "Epoch 34/100\n",
      "12/12 - 0s - loss: 0.4745 - accuracy: 0.7958\n",
      "Epoch 35/100\n",
      "12/12 - 0s - loss: 0.4721 - accuracy: 0.7906\n",
      "Epoch 36/100\n",
      "12/12 - 0s - loss: 0.4706 - accuracy: 0.7958\n",
      "Epoch 37/100\n",
      "12/12 - 0s - loss: 0.4674 - accuracy: 0.7958\n",
      "Epoch 38/100\n",
      "12/12 - 0s - loss: 0.4646 - accuracy: 0.7958\n",
      "Epoch 39/100\n",
      "12/12 - 0s - loss: 0.4625 - accuracy: 0.7958\n",
      "Epoch 40/100\n",
      "12/12 - 0s - loss: 0.4603 - accuracy: 0.7958\n",
      "Epoch 41/100\n",
      "12/12 - 0s - loss: 0.4579 - accuracy: 0.8010\n",
      "Epoch 42/100\n",
      "12/12 - 0s - loss: 0.4559 - accuracy: 0.8010\n",
      "Epoch 43/100\n",
      "12/12 - 0s - loss: 0.4540 - accuracy: 0.8063\n",
      "Epoch 44/100\n",
      "12/12 - 0s - loss: 0.4515 - accuracy: 0.8168\n",
      "Epoch 45/100\n",
      "12/12 - 0s - loss: 0.4496 - accuracy: 0.8168\n",
      "Epoch 46/100\n",
      "12/12 - 0s - loss: 0.4478 - accuracy: 0.8168\n",
      "Epoch 47/100\n",
      "12/12 - 0s - loss: 0.4453 - accuracy: 0.8220\n",
      "Epoch 48/100\n",
      "12/12 - 0s - loss: 0.4431 - accuracy: 0.8220\n",
      "Epoch 49/100\n",
      "12/12 - 0s - loss: 0.4416 - accuracy: 0.8168\n",
      "Epoch 50/100\n",
      "12/12 - 0s - loss: 0.4393 - accuracy: 0.8168\n",
      "Epoch 51/100\n",
      "12/12 - 0s - loss: 0.4376 - accuracy: 0.8168\n",
      "Epoch 52/100\n",
      "12/12 - 0s - loss: 0.4352 - accuracy: 0.8272\n",
      "Epoch 53/100\n",
      "12/12 - 0s - loss: 0.4336 - accuracy: 0.8272\n",
      "Epoch 54/100\n",
      "12/12 - 0s - loss: 0.4315 - accuracy: 0.8325\n",
      "Epoch 55/100\n",
      "12/12 - 0s - loss: 0.4292 - accuracy: 0.8325\n",
      "Epoch 56/100\n",
      "12/12 - 0s - loss: 0.4277 - accuracy: 0.8325\n",
      "Epoch 57/100\n",
      "12/12 - 0s - loss: 0.4256 - accuracy: 0.8325\n",
      "Epoch 58/100\n",
      "12/12 - 0s - loss: 0.4236 - accuracy: 0.8325\n",
      "Epoch 59/100\n",
      "12/12 - 0s - loss: 0.4218 - accuracy: 0.8325\n",
      "Epoch 60/100\n",
      "12/12 - 0s - loss: 0.4197 - accuracy: 0.8325\n",
      "Epoch 61/100\n",
      "12/12 - 0s - loss: 0.4176 - accuracy: 0.8377\n",
      "Epoch 62/100\n",
      "12/12 - 0s - loss: 0.4155 - accuracy: 0.8377\n",
      "Epoch 63/100\n",
      "12/12 - 0s - loss: 0.4144 - accuracy: 0.8325\n",
      "Epoch 64/100\n",
      "12/12 - 0s - loss: 0.4116 - accuracy: 0.8377\n",
      "Epoch 65/100\n",
      "12/12 - 0s - loss: 0.4094 - accuracy: 0.8377\n",
      "Epoch 66/100\n",
      "12/12 - 0s - loss: 0.4073 - accuracy: 0.8377\n",
      "Epoch 67/100\n",
      "12/12 - 0s - loss: 0.4052 - accuracy: 0.8325\n",
      "Epoch 68/100\n",
      "12/12 - 0s - loss: 0.4031 - accuracy: 0.8377\n",
      "Epoch 69/100\n",
      "12/12 - 0s - loss: 0.4010 - accuracy: 0.8429\n",
      "Epoch 70/100\n",
      "12/12 - 0s - loss: 0.3986 - accuracy: 0.8429\n",
      "Epoch 71/100\n",
      "12/12 - 0s - loss: 0.3967 - accuracy: 0.8482\n",
      "Epoch 72/100\n",
      "12/12 - 0s - loss: 0.3946 - accuracy: 0.8429\n",
      "Epoch 73/100\n",
      "12/12 - 0s - loss: 0.3931 - accuracy: 0.8482\n",
      "Epoch 74/100\n",
      "12/12 - 0s - loss: 0.3913 - accuracy: 0.8482\n",
      "Epoch 75/100\n",
      "12/12 - 0s - loss: 0.3887 - accuracy: 0.8482\n",
      "Epoch 76/100\n",
      "12/12 - 0s - loss: 0.3873 - accuracy: 0.8429\n",
      "Epoch 77/100\n",
      "12/12 - 0s - loss: 0.3853 - accuracy: 0.8429\n",
      "Epoch 78/100\n",
      "12/12 - 0s - loss: 0.3838 - accuracy: 0.8429\n",
      "Epoch 79/100\n",
      "12/12 - 0s - loss: 0.3816 - accuracy: 0.8482\n",
      "Epoch 80/100\n",
      "12/12 - 0s - loss: 0.3801 - accuracy: 0.8482\n",
      "Epoch 81/100\n",
      "12/12 - 0s - loss: 0.3784 - accuracy: 0.8482\n",
      "Epoch 82/100\n",
      "12/12 - 0s - loss: 0.3768 - accuracy: 0.8482\n",
      "Epoch 83/100\n",
      "12/12 - 0s - loss: 0.3750 - accuracy: 0.8534\n",
      "Epoch 84/100\n",
      "12/12 - 0s - loss: 0.3733 - accuracy: 0.8586\n",
      "Epoch 85/100\n",
      "12/12 - 0s - loss: 0.3712 - accuracy: 0.8534\n",
      "Epoch 86/100\n",
      "12/12 - 0s - loss: 0.3700 - accuracy: 0.8534\n",
      "Epoch 87/100\n",
      "12/12 - 0s - loss: 0.3682 - accuracy: 0.8534\n",
      "Epoch 88/100\n",
      "12/12 - 0s - loss: 0.3670 - accuracy: 0.8586\n",
      "Epoch 89/100\n",
      "12/12 - 0s - loss: 0.3652 - accuracy: 0.8534\n",
      "Epoch 90/100\n",
      "12/12 - 0s - loss: 0.3630 - accuracy: 0.8534\n",
      "Epoch 91/100\n",
      "12/12 - 0s - loss: 0.3615 - accuracy: 0.8534\n",
      "Epoch 92/100\n",
      "12/12 - 0s - loss: 0.3602 - accuracy: 0.8534\n",
      "Epoch 93/100\n",
      "12/12 - 0s - loss: 0.3583 - accuracy: 0.8534\n",
      "Epoch 94/100\n",
      "12/12 - 0s - loss: 0.3564 - accuracy: 0.8534\n",
      "Epoch 95/100\n",
      "12/12 - 0s - loss: 0.3548 - accuracy: 0.8534\n",
      "Epoch 96/100\n",
      "12/12 - 0s - loss: 0.3531 - accuracy: 0.8534\n",
      "Epoch 97/100\n",
      "12/12 - 0s - loss: 0.3520 - accuracy: 0.8534\n",
      "Epoch 98/100\n",
      "12/12 - 0s - loss: 0.3509 - accuracy: 0.8586\n",
      "Epoch 99/100\n",
      "12/12 - 0s - loss: 0.3483 - accuracy: 0.8534\n",
      "Epoch 100/100\n",
      "12/12 - 0s - loss: 0.3473 - accuracy: 0.8586\n",
      "Accuracy: 67.37\n"
     ]
    }
   ],
   "source": [
    "# example of one hot encoding for a neural network\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(filename):\n",
    "\t# load the dataset as a pandas DataFrame\n",
    "\tdata = read_csv(filename, header=None)\n",
    "\t# retrieve numpy array\n",
    "\tdataset = data.values\n",
    "\t# split into input (X) and output (y) variables\n",
    "\tX = dataset[:, :-1]\n",
    "\ty = dataset[:,-1]\n",
    "\t# format all fields as string\n",
    "\tX = X.astype(str)\n",
    "\t# reshape target to be a 2d array\n",
    "\ty = y.reshape((len(y), 1))\n",
    "\treturn X, y\n",
    "\n",
    "# prepare input data\n",
    "def prepare_inputs(X_train, X_test):\n",
    "\tohe = OneHotEncoder()\n",
    "\tohe.fit(X_train)\n",
    "\tX_train_enc = ohe.transform(X_train)\n",
    "\tX_test_enc = ohe.transform(X_test)\n",
    "\treturn X_train_enc, X_test_enc\n",
    "\n",
    "# prepare target\n",
    "def prepare_targets(y_train, y_test):\n",
    "\tle = LabelEncoder()\n",
    "\tle.fit(y_train)\n",
    "\ty_train_enc = le.transform(y_train)\n",
    "\ty_test_enc = le.transform(y_test)\n",
    "\treturn y_train_enc, y_test_enc\n",
    "\n",
    "# load the dataset\n",
    "X, y = load_dataset('../input/breast-cancer.csv')\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# prepare input data\n",
    "X_train_enc, X_test_enc = prepare_inputs(X_train, X_test)\n",
    "# prepare output data\n",
    "y_train_enc, y_test_enc = prepare_targets(y_train, y_test)\n",
    "# define the  model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=X_train_enc.shape[1], activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X_train_enc, y_train_enc, epochs=100, batch_size=16, verbose=2)\n",
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(X_test_enc, y_test_enc, verbose=0)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "12/12 - 1s - loss: 0.6886 - accuracy: 0.6126\n",
      "Epoch 2/20\n",
      "12/12 - 0s - loss: 0.6656 - accuracy: 0.7277\n",
      "Epoch 3/20\n",
      "12/12 - 0s - loss: 0.6384 - accuracy: 0.7277\n",
      "Epoch 4/20\n",
      "12/12 - 0s - loss: 0.6093 - accuracy: 0.7277\n",
      "Epoch 5/20\n",
      "12/12 - 0s - loss: 0.5866 - accuracy: 0.7277\n",
      "Epoch 6/20\n",
      "12/12 - 0s - loss: 0.5662 - accuracy: 0.7277\n",
      "Epoch 7/20\n",
      "12/12 - 0s - loss: 0.5543 - accuracy: 0.7277\n",
      "Epoch 8/20\n",
      "12/12 - 0s - loss: 0.5452 - accuracy: 0.7277\n",
      "Epoch 9/20\n",
      "12/12 - 0s - loss: 0.5375 - accuracy: 0.7277\n",
      "Epoch 10/20\n",
      "12/12 - 0s - loss: 0.5299 - accuracy: 0.7277\n",
      "Epoch 11/20\n",
      "12/12 - 0s - loss: 0.5227 - accuracy: 0.7330\n",
      "Epoch 12/20\n",
      "12/12 - 0s - loss: 0.5167 - accuracy: 0.7487\n",
      "Epoch 13/20\n",
      "12/12 - 0s - loss: 0.5100 - accuracy: 0.7487\n",
      "Epoch 14/20\n",
      "12/12 - 0s - loss: 0.5035 - accuracy: 0.7592\n",
      "Epoch 15/20\n",
      "12/12 - 0s - loss: 0.4976 - accuracy: 0.7644\n",
      "Epoch 16/20\n",
      "12/12 - 0s - loss: 0.4919 - accuracy: 0.7801\n",
      "Epoch 17/20\n",
      "12/12 - 0s - loss: 0.4869 - accuracy: 0.7853\n",
      "Epoch 18/20\n",
      "12/12 - 0s - loss: 0.4811 - accuracy: 0.7906\n",
      "Epoch 19/20\n",
      "12/12 - 0s - loss: 0.4769 - accuracy: 0.7853\n",
      "Epoch 20/20\n",
      "12/12 - 0s - loss: 0.4738 - accuracy: 0.7853\n",
      "Accuracy: 71.58\n"
     ]
    }
   ],
   "source": [
    "# example of learned embedding encoding for a neural network\n",
    "from numpy import unique\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(filename):\n",
    "\t# load the dataset as a pandas DataFrame\n",
    "\tdata = read_csv(filename, header=None)\n",
    "\t# retrieve numpy array\n",
    "\tdataset = data.values\n",
    "\t# split into input (X) and output (y) variables\n",
    "\tX = dataset[:, :-1]\n",
    "\ty = dataset[:,-1]\n",
    "\t# format all fields as string\n",
    "\tX = X.astype(str)\n",
    "\t# reshape target to be a 2d array\n",
    "\ty = y.reshape((len(y), 1))\n",
    "\treturn X, y\n",
    "\n",
    "# prepare input data\n",
    "def prepare_inputs(X_train, X_test):\n",
    "\tX_train_enc, X_test_enc = list(), list()\n",
    "\t# label encode each column\n",
    "\tfor i in range(X_train.shape[1]):\n",
    "\t\tle = LabelEncoder()\n",
    "\t\tle.fit(X_train[:, i])\n",
    "\t\t# encode\n",
    "\t\ttrain_enc = le.transform(X_train[:, i])\n",
    "\t\ttest_enc = le.transform(X_test[:, i])\n",
    "\t\t# store\n",
    "\t\tX_train_enc.append(train_enc)\n",
    "\t\tX_test_enc.append(test_enc)\n",
    "\treturn X_train_enc, X_test_enc\n",
    "\n",
    "# prepare target\n",
    "def prepare_targets(y_train, y_test):\n",
    "\tle = LabelEncoder()\n",
    "\tle.fit(y_train)\n",
    "\ty_train_enc = le.transform(y_train)\n",
    "\ty_test_enc = le.transform(y_test)\n",
    "\treturn y_train_enc, y_test_enc\n",
    "\n",
    "# load the dataset\n",
    "X, y = load_dataset('../input/breast-cancer.csv')\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# prepare input data\n",
    "X_train_enc, X_test_enc = prepare_inputs(X_train, X_test)\n",
    "# prepare output data\n",
    "y_train_enc, y_test_enc = prepare_targets(y_train, y_test)\n",
    "# make output 3d\n",
    "y_train_enc = y_train_enc.reshape((len(y_train_enc), 1, 1))\n",
    "y_test_enc = y_test_enc.reshape((len(y_test_enc), 1, 1))\n",
    "# prepare each input head\n",
    "in_layers = list()\n",
    "em_layers = list()\n",
    "for i in range(len(X_train_enc)):\n",
    "\t# calculate the number of unique inputs\n",
    "\tn_labels = len(unique(X_train_enc[i]))\n",
    "\t# define input layer\n",
    "\tin_layer = Input(shape=(1,))\n",
    "\t# define embedding layer\n",
    "\tem_layer = Embedding(n_labels, 10)(in_layer)\n",
    "\t# store layers\n",
    "\tin_layers.append(in_layer)\n",
    "\tem_layers.append(em_layer)\n",
    "# concat all embeddings\n",
    "merge = concatenate(em_layers)\n",
    "dense = Dense(10, activation='relu', kernel_initializer='he_normal')(merge)\n",
    "output = Dense(1, activation='sigmoid')(dense)\n",
    "model = Model(inputs=in_layers, outputs=output)\n",
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# plot graph\n",
    "plot_model(model, show_shapes=True, to_file='embeddings.png')\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X_train_enc, y_train_enc, epochs=20, batch_size=16, verbose=2)\n",
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(X_test_enc, y_test_enc, verbose=0)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project .\\my_project\\oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from .\\my_project\\tuner0.json\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "import keras_tuner as kt\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn import pipeline\n",
    "\n",
    "def build_model(hp):\n",
    "  model_type = hp.Choice('model_type', ['random_forest', 'ridge'])\n",
    "  if model_type == 'random_forest':\n",
    "    model = ensemble.RandomForestClassifier(\n",
    "        n_estimators=hp.Int('n_estimators', 10, 50, step=10),\n",
    "        max_depth=hp.Int('max_depth', 3, 10))\n",
    "  else:\n",
    "    model = linear_model.RidgeClassifier(\n",
    "        alpha=hp.Float('alpha', 1e-3, 1, sampling='log'))\n",
    "  return model\n",
    "\n",
    "tuner = kt.tuners.SklearnTuner(\n",
    "    oracle=kt.oracles.BayesianOptimizationOracle(\n",
    "        objective=kt.Objective('score', 'max'),\n",
    "        max_trials=10),\n",
    "    hypermodel=build_model,\n",
    "    scoring=metrics.make_scorer(metrics.accuracy_score),\n",
    "    cv=model_selection.StratifiedKFold(5),\n",
    "    directory='.',\n",
    "    project_name='my_project')\n",
    "\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y, test_size=0.2)\n",
    "\n",
    "tuner.search(X_train, y_train)\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'to_categorical' from 'keras.utils' (C:\\Users\\furyx\\miniconda3\\envs\\tf27\\lib\\site-packages\\keras\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_31572/628648181.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmake_blobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'to_categorical' from 'keras.utils' (C:\\Users\\furyx\\miniconda3\\envs\\tf27\\lib\\site-packages\\keras\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# global optimization to find coefficients for weighted ensemble on blobs problem\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from matplotlib import pyplot\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from numpy import tensordot\n",
    "from numpy.linalg import norm\n",
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "# fit model on dataset\n",
    "def fit_model(trainX, trainy):\n",
    "\ttrainy_enc = to_categorical(trainy)\n",
    "\t# define model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(25, input_dim=2, activation='relu'))\n",
    "\tmodel.add(Dense(3, activation='softmax'))\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\t# fit model\n",
    "\tmodel.fit(trainX, trainy_enc, epochs=500, verbose=0)\n",
    "\treturn model\n",
    "\n",
    "# make an ensemble prediction for multi-class classification\n",
    "def ensemble_predictions(members, weights, testX):\n",
    "\t# make predictions\n",
    "\tyhats = [model.predict(testX) for model in members]\n",
    "\tyhats = array(yhats)\n",
    "\t# weighted sum across ensemble members\n",
    "\tsummed = tensordot(yhats, weights, axes=((0),(0)))\n",
    "\t# argmax across classes\n",
    "\tresult = argmax(summed, axis=1)\n",
    "\treturn result\n",
    "\n",
    "# # evaluate a specific number of members in an ensemble\n",
    "def evaluate_ensemble(members, weights, testX, testy):\n",
    "\t# make prediction\n",
    "\tyhat = ensemble_predictions(members, weights, testX)\n",
    "\t# calculate accuracy\n",
    "\treturn accuracy_score(testy, yhat)\n",
    "\n",
    "# normalize a vector to have unit norm\n",
    "def normalize(weights):\n",
    "\t# calculate l1 vector norm\n",
    "\tresult = norm(weights, 1)\n",
    "\t# check for a vector of all zeros\n",
    "\tif result == 0.0:\n",
    "\t\treturn weights\n",
    "\t# return normalized vector (unit norm)\n",
    "\treturn weights / result\n",
    "\n",
    "# loss function for optimization process, designed to be minimized\n",
    "def loss_function(weights, members, testX, testy):\n",
    "\t# normalize weights\n",
    "\tnormalized = normalize(weights)\n",
    "\t# calculate error rate\n",
    "\treturn 1.0 - evaluate_ensemble(members, normalized, testX, testy)\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "print(trainX.shape, testX.shape)\n",
    "# fit all models\n",
    "n_members = 5\n",
    "members = [fit_model(trainX, trainy) for _ in range(n_members)]\n",
    "# evaluate each single model on the test set\n",
    "testy_enc = to_categorical(testy)\n",
    "for i in range(n_members):\n",
    "\t_, test_acc = members[i].evaluate(testX, testy_enc, verbose=0)\n",
    "\tprint('Model %d: %.3f' % (i+1, test_acc))\n",
    "# evaluate averaging ensemble (equal weights)\n",
    "weights = [1.0/n_members for _ in range(n_members)]\n",
    "score = evaluate_ensemble(members, weights, testX, testy)\n",
    "print('Equal Weights Score: %.3f' % score)\n",
    "# define bounds on each weight\n",
    "bound_w = [(0.0, 1.0)  for _ in range(n_members)]\n",
    "# arguments to the loss function\n",
    "search_arg = (members, testX, testy)\n",
    "# global optimization of ensemble weights\n",
    "result = differential_evolution(loss_function, bound_w, search_arg, maxiter=1000, tol=1e-7)\n",
    "# get the chosen weights\n",
    "weights = normalize(result['x'])\n",
    "print('Optimized Weights: %s' % weights)\n",
    "# evaluate chosen weights\n",
    "score = evaluate_ensemble(members, weights, testX, testy)\n",
    "print('Optimized Weights Score: %.3f' % score)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1df55bfbe440201d91a0da14712334519d9d02eb88f6d983f6035b8a92006195"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('tf2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
